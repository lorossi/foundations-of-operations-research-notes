\documentclass[english]{article}
\usepackage[title={Foundations of Operation Research}, date={2022/2023}]{notestemplate}
\usepackage{for}

\begin{document}

\makecover

\section{Introduction}

\subsection{Algorithm}

An algorithm for a problem is a sequence of instructions that allows to solve any of its instances.
The execution time of an algorithm depends on various factors, most notably the instance and the computer.

\subparagraph*{Properties}

\begin{itemize}
  \item An algorithm is \textbf{exact} if it provides an optimal solution for every instance.
        \begin{itemize}
          \item otherwise is \textbf{heuristic}
        \end{itemize}
  \item A \textbf{greedy algorithm} constructs a feasible solution iteratively, by making at each step a \textit{locally optimal} choice, without reconsidering previous choices
        \begin{itemize}
          \item for most \textit{discrete optimization problems}, greedy type algorithms yield a feasible solution with no guarantee of optimality
        \end{itemize}
\end{itemize}

\subsection{Dynamic Programming}

Proposed by \textit{Richard Bellman} in \(1950\), \textbf{dynamic programming} \textit{(or \DP)} is a method for solving optimization problems, composed of a sequence of decisions, by solving a set of recursive equations.

\DP is applicable to any sequential decision problem, for which the optimality property is satisfied;
as such, it has a wide range of applications, including scheduling, transportation, and assignment problems.

\subsection{Complexity of algorithms}

In order analyze an algorithm, it's necessary to consider its complexity as a function of the size of the instance \textit{(the size of the input)}, independently of the computer;
the complexity is defined as the number of elementary operations by assuming that each elementary operation takes a constant time.

Since it's hard to determine the exact number of elementary operations, an additional assumption is made: only the asymptotic number of elementary operations in the worst case (for the worst instances) is considered.
The complexity evaluation is then performed by looking for the function \(f(n)\) that best approximates the upper bound on the number of elementary operations \(n\) for the worst instances.

\subsubsection{Big-O notation}

A function \(f\) if order of \(g\), written \(f(n) = \bigO{g(n)}\) if exists a constant \(c > 0\) and a constant \(n_0 > 0\) such that \(f(n) \leq c \cdot g(n)\) for all \(n \geq n_0\).

An illustration of the big-O notation is shown in Figure~\ref{fig:big-o-notation}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{figure-1.tikz}
  \caption{Big-O notation}
  \label{fig:big-o-notation}
  \bigskip
\end{figure}

\subsection{Complexity classes}

Two classes of algorithms are considered, according to their worst case order of complexity:

\begin{itemize}
  \item \textbf{Polynomial}: \(\bigO{n^d}\) for a constant \(d > 0, d \in \R\)
  \item \textbf{Exponential}: \(\bigO{d^n}\) for a constant \(d > 0, d \in \R\)
\end{itemize}

Algorithms with a high order Polynomial complexity are not considered efficient;
a comparison of the two classes, assuming that \(1 \, \mu s\) is needed for each elementary operation, is shown in Table~\ref{tab:complexity-classes}.

\begin{table}[htbp]
  \centering
  \bigskip
  \begin{tblr}{colspec={c|c|c}}
    \(n\)    & \(n^2\)          & \(2^n\)                               \\
    \hline
    \(1\)    & \(1 \, \mu s\)   & \(1 \, \mu s\)                        \\
    \(5\)    & \(25 \, \mu s\)  & \(32 \, \mu s\)                       \\
    \(10\)   & \(100 \, \mu s\) & \(1.024 \, ms\)                       \\
    \(20\)   & \(400 \, \mu s\) & \(\approx 1.04\, s\)                  \\
    \(30\)   & \(900 \, \mu s\) & \(\approx 18 \, m\)                   \\
    \(40\)   & \(1.6 \, ms\)    & \(\approx 13 \, d\)                   \\
    \(50\)   & \(2.5 \, ms\)    & \(\approx 36 \, y\)                   \\
    \(100\)  & \(10 \, ms\)     & \(\approx 4.017 \cdot 10^{16} \, y\)  \\
    \(1000\) & \(1 \, s\)       & \(\approx 3.395 \cdot 10^{287} \, y\) \\
  \end{tblr}
  \caption{Complexity classes}
  \label{tab:complexity-classes}
  \bigskip
\end{table}

\clearpage

\section{Graph and Network Optimization}

Many \textbf{decision making problems} can be formulated in terms of graphs and networks, such as:

\begin{itemize}
  \item \textbf{transportation} and \textbf{distribution} problems
  \item \textbf{network design} problems
  \item \textbf{location} problem
  \item timetable \textbf{scheduling}
  \item \ldots
\end{itemize}

\subsection{Graphs}

A \textbf{graph} is a pair \(G = (N, E)\) with:

\begin{itemize}
  \item \(N\) a set of \textbf{nodes} or \textbf{vertices}
  \item \(E \subseteq N \times N\) a set of \textbf{edges} or arcs connecting them pairwise
        \begin{itemize}
          \item A \textbf{directed} graph is a graph in which set set \(E\) is composed by ordered pairs of distinct nodes
                \begin{itemize}[label=\(\rightarrow\)]
                  \item an edge connecting nodes \(i\) and \(j\) is represented by \(\{i, j\}\)
                  \item the flow is permitted only in the direction of the arrow \textit{(from \(i\) to \(j\))}
                  \item \(i\) is the head of the arc and \(j\) is the tail
                \end{itemize}
          \item A \textbf{undirected} graph is a graph in which set set \(E\) is composed by unordered pairs of distinct nodes
                \begin{itemize}[label=\(\rightarrow\)]
                  \item an edge connecting nodes \(i\) and \(j\) is represented by \((i, j)\)
                  \item since all edges are undirected, \((i, j)\) is equivalent to \((j, i)\)
                \end{itemize}
        \end{itemize}
\end{itemize}

\subparagraph*{Properties}

\begin{itemize}
  \item Two \textbf{nodes} \(i\) and \(j\) are \textbf{adjacent} if they are connected by an edge \((i, j)\) or \(\{i, j\}\)
  \item An \textbf{edge} \(e\) is \textbf{incident} in a node \(v\) if \(v\) is an \textbf{endpoint} of \(e\)
        \begin{itemize}
          \item \textbf{undirected} graphs: the degree of a node is the number of incident edges
          \item \textbf{directed} graphs: the in-degree \textit{(out-degree)} of a node is the number of arcs that have it as successor \textit{(predecessor)}
        \end{itemize}
  \item A \textbf{path} from \(i \in N \text{ to } j \in N\) is a sequence of edges in a undirected graph
        \[ p = \left\langle \{v_1, v_2\}, \{v_2, v_3\}, \, \ldots \,, \{v_{k-1}, v_k\}\right\rangle \]
        connecting nodes \(v_1, \, \ldots \,, v_k\), with \(\{v_i, v_{i+1} \in E\}\) for \(i = 1, \, \ldots \,, k-1\)
  \item A \textbf{directed path} \(i \in N \text{ to } j \in N\) is a sequence of arcs in a directed graph
        \[ p = \left\langle (v_1, v_2), (v_2, v_3), \, \ldots \,, (v_{k-1}, v_k)\right\rangle \]
        connecting nodes, with  \((v_i, v_{i+1} \in E)\)  for \(i = 1, \, \ldots \,, k-1\)
  \item A \textbf{cycle} is a path \[ \left\langle v_1, v_2, \, \ldots \,, v_1 \right\rangle \] where the first and last nodes are the same
        \begin{itemize}
          \item A \textbf{directed cycle} is path \[ \left\langle \{v_1, v_2\}, \{v_2, v_3\}, \, \ldots \,, \{v_{k-1}, v_k\}, \{v_k, v_1\}\right\rangle \]
                where the first and the last nodes are the same
        \end{itemize}
  \item Nodes \(u\) and \(v\) are \textbf{connected} if exists a path connecting them
  \item A graph \((N, E)\) is \textbf{connected} if \(u, v\) are connecting \(\forall \, u, v \in N\)
        \begin{itemize}
          \item A graph \((N, E)\) is \textbf{strongly connected} if \(u, v\) are connected by a \textbf{directed} path \(\forall \, u, v \in N\)
        \end{itemize}
  \item A graph is \textbf{bipartite} if there is a partition \(N = N_1 \cup N_2, \, N_1 \cap N_2 = \emptyset\) such that \(\forall \, (u, v) \in E, u \in N_1 \text{ and } v \in N_2\)
  \item A graph is \textbf{complete} if \(E = \left\{\{v_i, v_j\} \,|\, v_i, v_j \in N \land i \leq j \right\}\)
  \item Given a directed graph \(G = (N, A)\) and \(S \subseteq N\), the \textbf{outgoing cut} induced by \(S\) is the set of arcs:
        \[\delta^+(S) = \left\{ (u, v) \in A \,|\, u \in S \land v \in N \setminus S \right\}\]
        the \textbf{incoming cut} induced by \(S\) is the set of arcs:
        \[\delta^-(S) = \left\{ (u, v) \in A \,|\, v \in S \land u \in N \setminus S \right\}\]
  \item A graph with \(n\) \textbf{nodes} has at most \(m = \dfrac{n(n-1)}{2}\) \textbf{edges}
  \item A \textbf{directed} graph with \(n\) \textbf{nodes} has at most \(m = n(n-1)\) \textbf{arcs}
        \begin{itemize}
          \item a graph is \textbf{dense} if \(m \approx n^2\)
          \item a graph is \textbf{sparse} if \(m \ll n\)
        \end{itemize}
  \item The adjacency list \(A(i)\) of a node \(i\) is the set of arcs emanating from that node
        \[ A(i) = \left\{ j \in N \mid   (i, j) \in A  \right\} \]
\end{itemize}

\bigskip
Some examples are shown in Figure \ref{fig:examples-of-graphs}.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[node distance = 2cm, minimum size = 0.75cm]
      \samplegraph
      \sampleedges
    \end{tikzpicture}
    \caption{Undirected graph}
    \label{fig:undirected-graph}
  \end{subfigure}
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[node distance = 2cm, minimum size = 0.75cm]
      \samplegraph
      \samplearcs
    \end{tikzpicture}
    \caption{Directed graph}
    \label{subfig:directed-graph}
  \end{subfigure}
  \bigskip
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[node distance = 2cm, minimum size = 0.75cm]
      \samplegraph

      \path[-] (1) edge (2);
      \path[-] (1) edge (4);
      \path[-][color=red] (2) edge (3);
      \path[-] (2) edge (4);
      \path[-][color=red] (3) edge (4);
      \path[-] (3) edge (5);
      \path[-][color=red] (4) edge (5);
    \end{tikzpicture}
    \caption{\shortstack{Connected graph, nodes \(2\) and \(5\) are connected \\ \(\langle \{2, 3\}, \{3, 4\}, \{4, 5\} \rangle\) is a path}}
    \label{subfig:connected-graph}
  \end{subfigure}
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \samplegraph

      \path[->] (1) edge (2);
      \path[->] (1) edge (4);
      \path[->, color=red] (2) edge (3);
      \path[->, color=red] (3) edge (4);
      \path[->, color=red] (3) edge (5);
      \path[->, color=red] (4) edge (2);
      \path[->] (4) edge[bend left=15] (5);
      \path[->, color=red] (5) edge[bend left=15] (4);
    \end{tikzpicture}
    \caption{\shortstack{Not strongly connected graph \\ \(\langle \{3, 5\}, \{5, 4\}, \{4, 2\}, \{2, 3\}, \{3, 4\} \rangle\) is a directed path}}
    \label{subfig:not-strongly-connected-graph}
  \end{subfigure}
  \bigskip
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \samplegraph

      \path[-] (1) edge (2);
      \path[-] (1) edge (4);
      \path[-, color=red] (2) edge (3);
      \path[-, color=red] (2) edge (4);
      \path[-] (3) edge (4);
      \path[-, color=red] (3) edge (5);
      \path[-, color=red] (4) edge (5);
    \end{tikzpicture}
    \caption{\shortstack{Cycle \\ \(\langle \{2, 3\}, \{3, 5\}, \{5, 4\}, \{4, 2\}\rangle\) is a cycle}}
    \label{subfig:cycle}
  \end{subfigure}
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \samplegraph

      \path[->] (1) edge (2);
      \path[->] (1) edge (4);
      \path[->, color=red] (2) edge (3);
      \path[->, color=red] (3) edge (4);
      \path[->] (3) edge (5);
      \path[->, color=red] (4) edge (2);
      \path[->] (4) edge[bend left=15] (5);
      \path[->] (5) edge[bend left=15] (4);
    \end{tikzpicture}
    \caption{\shortstack{Circuit \\ \(\langle (2, 3), (3, 4), (4, 2)\)} is a circuit}
    \label{subfig:circuit}
  \end{subfigure}
  \bigskip
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \samplegraph

      \path[-] (3) edge (5);
      \path[-] (5) edge (1);
      \path[-] (1) edge (4);
      \path[-] (4) edge (2);
    \end{tikzpicture}
    \caption{\shortstack{Bipartite graph \\ \(N_1 = \left\{1, 2, 3\right\}, N_2 = \left\{4, 5\right\}\)}}
    \label{subfig:bipartite-graph}
  \end{subfigure}
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \samplegraph

      \path[-] (2) edge (3);
      \path[-] (3) edge (4);
      \path[-] (4) edge (2);
      \path[-] (1) edge (2);
      \path[-] (1) edge (3);
      \path[-] (1) edge (4);
    \end{tikzpicture}
    \caption{\shortstack{Complete graph \\ \(N = \left\{1, 2, 3, 4\right\}\)}}
    \label{subfig:complete-graph}
  \end{subfigure}
  \bigskip
  \begin{subfigure}[t]{0.99\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \node[draw, fill=purple, draw=purple, text=white] (2) {2};
      \node[draw, fill=purple, draw=purple, text=white] (3) [right of = 2] {3};
      \node[draw, fill=purple, draw=purple, text=white] (5) [right of = 3] {5};
      \node[draw, fill=blue, draw=blue, text=white] (1) [below of = 2] {1};
      \node[draw, fill=blue, draw=blue, text=white] (4) [below of = 3] {4};

      \path[->, color=blue] (1) edge (2);
      \path[->, color=blue] (4) edge (2);
      \path[->, color=blue] (4) edge [bend left=15] (5);

      \path[->, color=purple] (3) edge (4);
      \path[->, color=purple] (5) edge [bend left=15] (4);

      \path[->] (2) edge (3);
      \path[->] (3) edge (5);
      \path[->] (1) edge (4);
    \end{tikzpicture}
    \caption{incoming (\(\delta^+\)) and outgoing (\(\delta^-\)) cuts of two sets of nodes \textit{(purple and blue)}}
    \label{subfig:incoming-outgoing-cuts}
  \end{subfigure}
  \caption{Examples of graphs}
  \label{fig:examples-of-graphs}
\end{figure}

\subsubsection{Graphs representation}

Graphs are represented by:

\begin{itemize}
  \item Adjacency \textbf{matrix} \(\mathbf{A}\) of size \(n \times n\) if the graph is \textbf{dense}
        \[ a_{ij} = \begin{cases} 1 \ &\text{if } (i, j) \in A\\ a_{i_j} &\text{otherwise}\end{cases} \]
  \item Adjacency \textbf{list} \(\mathbf{A}\) of size \(n\) if the graph is \textbf{sparse}
\end{itemize}

The same representation can be used for both directed and undirected graphs;
the adjacency matrix for an undirected graph is \textbf{symmetric}.

\bigskip
An example of a graph representation is shown in Figure~\ref{fig:graph-representation}.

\begin{figure}[htbp]
  \begin{subfigure}{0.33\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \samplegraph

      \path[->] (1) edge (2);
      \path[->] (1) edge (4);
      \path[->] (2) edge (3);
      \path[->] (3) edge (4);
      \path[->] (3) edge (5);
      \path[->] (4) edge (2);
      \path[->] (4) edge[bend left=15] (5);
      \path[->] (5) edge[bend left=15] (4);
    \end{tikzpicture}
    \caption{Graph}
  \end{subfigure}
  \begin{subfigure}{0.33\textwidth}
    \centering
    \(A = \begin{bmatrix}
      0 & 1 & 0 & 1 & 0 \\
      0 & 0 & 1 & 0 & 0 \\
      0 & 0 & 0 & 1 & 1 \\
      0 & 1 & 0 & 0 & 1 \\
      0 & 0 & 0 & 1 & 0
    \end{bmatrix}\)
    \caption{Adjacency matrix}
  \end{subfigure}
  \begin{subfigure}{0.33\textwidth}
    \centering
    \(\begin{matrix}
      S(1) = \left\{2, 4\right\} \\
      S(2) = \left\{3\right\}    \\
      S(3) = \left\{4, 5\right\} \\
      S(4) = \left\{2, 5\right\} \\
      S(5) = \left\{4\right\}
    \end{matrix}\)
    \caption{Adjacency list}
  \end{subfigure}
  \caption{Graph representation}
  \label{fig:graph-representation}
\end{figure}

\subsubsection{Graph reachability problem}

\begin{problem}[Graph reachability]
Given a directed graph \(G = (N, A)\) and a node \(s \in N\), the \textbf{graph reachability problem} consists in finding all nodes reachable from \(s\).
\end{problem}

\subparagraph*{Goal}
\begin{itemize}[label=\(\rightarrow\)]
  \item \textit{Input}: graph \(G = (N, A)\), described via successor lists, and a node \(s \in N\)
  \item \textit{Output}: subset \(M \subseteq N\) of nodes of \(G\) reachable from \(s\)
\end{itemize}

The goal is reached by an \textit{efficient} algorithm to solve the problem, with the following properties:
\begin{itemize}
  \item a \textbf{queue} \(Q\) of nodes not yet processed is kept by the algorithm
  \item the queue uses a \texttt{FIFO} policy
  \item the nodes exploration is performed in a \textbf{breadth-first} manner
\end{itemize}

\subparagraph*{Algorithm}
The algorithm pseudocode is shown in Code~\ref{lst:graph-reachability}.

\begin{lstlisting}[caption={Graph reachability}, label={lst:graph-reachability}, float]
Q := {s}
M := {}
while Q is not empty do
  u := node $\in$ Q
  Q := Q \ {u}
  M := M $\cup$ {u}
  for (u, v) $\in \ \delta^+(\texttt{u})$ do
    if v $\notin$ M and v $\notin$ Q then
      Q := Q $\cup$ {v}
    end
  end
end
\end{lstlisting}

The algorithm stops when \(\delta^+(M) = \emptyset\) \textit{(when the outgoing cut of the set of nodes \(M\) is empty)};
\(\delta^-(M)\) is the set of arcs with head node in \(M\) and tail in \(N \setminus M\).

\paragraph{Complexity analysis}

At each iteration of the \texttt{\textbf{while}} loop:

\begin{enumerate}
  \item A node \(u\) is \textbf{removed} from the queue \(Q\) and \textbf{added} to the set \(M\)
  \item For all nodes \(v\) directly reachable from \(u\) and not already in \(M\) or \(Q\), \(v\) is added to \(Q\)
\end{enumerate}

Since each node \(u\) is inserted in \(Q\) at most once and each arch \((u,v)\) is considered at most once, the overall complexity is:
\[ \bigO{n+m} \quad n = |N|, \ m =|A|\]
For dense graphs, this value converges to \(\bigO{n^2}\).

\subsection{Subgraphs and Trees}

Let \(G = (N, E)\) be a graph.
Then:

\begin{itemize}
  \item \(G^T = (N^\prime, E^\prime)\) is a \textbf{subgraph} of \(G\) if \(N^\prime \subseteq N\) and \(E^\prime \subseteq E\)
  \item A \textbf{tree} \(G_T = (N^\prime, T)\) of \(G\) is a connected, acyclic, subgraph of \(G\)
  \item \(G_T = (N^\prime, T)\) is a \textbf{spanning tree} of \(G\) if it contains all the nodes \textit{(\(N^\prime = N\))}
  \item The \textbf{leaves} of a tree are the nodes with degree \(1\)
\end{itemize}

\bigskip
A representation of these concepts is shown in Figure~\ref{fig:subgraphs-trees}.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.995\textwidth}
    \centering
    \begin{tikzpicture}[node distance = 2cm, minimum size = 0.75cm]
      \samplegraph
      \sampleedges
    \end{tikzpicture}
    \caption{Graph \(G\)}
  \end{subfigure}
  \bigskip
  \begin{subfigure}[t]{0.3\textwidth}
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \node[draw] (2) {2};
      \node[] (3) [right of = 2] {};
      \node[draw] (5) [right of = 3] {5};
      \node[draw] (1) [below of = 2] {1};
      \node[draw] (4) [below of = 3] {4};

      \path[-] (2) edge (4);
      \path[-] (4) edge (1);
      \path[-] (1) edge (2);
    \end{tikzpicture}
    \caption{Subgraph \(G^\prime\) of \(G\)}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \node[draw] (2) {2};
      \node[] (3) [right of = 2] {};
      \node[draw] (5) [right of = 3] {5};
      \node[draw] (1) [below of = 2] {1};
      \node[draw] (4) [below of = 3] {4};

      \path[-] (5) edge (4);
      \path[-] (4) edge (1);
      \path[-] (1) edge (2);
    \end{tikzpicture}
    \caption{Subgraph \(G^{\prime\prime}\) of \(G\) and a tree}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \samplegraph

      \path[-] (1) edge (4);
      \path[-] (2) edge (4);
      \path[-] (2) edge (3);
      \path[-] (4) edge (5);
    \end{tikzpicture}
    \caption{Subgraph \(G^{\prime\prime}\) of \(G\) and a tree}
  \end{subfigure}
  \caption{Subgraphs and trees}
  \label{fig:subgraphs-trees}
\end{figure}

\subsection{Properties of trees}

\begin{property}[Number of edges]
  Every tree with \(n\) nodes has \(n-1\) edges.
\end{property}

\begin{proof}
  \hfill
  \begin{itemize}
    \item \textbf{Base case}: the claim holds for \(n=1\) \textit{(a tree with a single node has no edges)}
    \item \textbf{Inductive steps}: show that the claim is valid for for any tree with \(n + 1\) nodes
          \begin{itemize}
            \item let \(T_1\) be a tree with \(n+1\) and recall with any tree with \(n \geq 2\) nodes has at least \(2\) leaves
            \item by deleting one of the leaves and its incident edge, a tree \(T_2\) with \(n\) nodes is obtained
            \item by induction hypothesis, \(T_2\) has \(n-1\) edges; therefore, \(T_1\) has \(n-1+1=n\) edges
          \end{itemize}
  \end{itemize}
\end{proof}

\begin{property}
  Any pair of nodes in a tree is connected via a unique path.
  Otherwise, the tree would contain a cycle.
  \label{prop:number-of-paths}
\end{property}

\begin{property}
  By adding a new edge to a tree, a new unique cycle is created.
  This cycle consists of the path created in Property~\ref{prop:number-of-paths} and the new edge.
\end{property}

\begin{property}
  Let \(G_T = (N, T)\) be a spanning tree of \(G = (N, E)\).
  Consider an edge \(e \notin T\) and the unique cycle \(C\) of \(T \cup \{e\}\).
  For each edge \(f \in C \setminus \{e\}\), the subgraph \(T \cup \{e\} \setminus \{f\}\) is a spanning tree of \(G\).
\end{property}

\begin{figure}[htbp]
  \centering
  \bigskip
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \samplegraph
      \path[-] (1) edge (2);
      \path[-] (2) edge (3);
      \path[-, text=red, color=red] (3) edge node[above]{\(f\)} (5);
      \path[-] (5) edge (4);
      \path[-, text=blue,color=blue] (4) edge node[above]{\(e\)} (2);
    \end{tikzpicture}
    \caption{Graph \(G_T\), edge \(f\) is red, edge \(e\) is blue}
  \end{subfigure}
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \samplegraph

      \path[-] (1) edge (2);
      \path[-] (2) edge (3);
      \path[-] (3) edge (5);
      \path[-] (5) edge (4);
    \end{tikzpicture}
    \caption{\(T \cup \{e\} \setminus \{f\}\)}
  \end{subfigure}
  \caption{Exchange property}
  \label{fig:exchange-property}
  \bigskip
\end{figure}

\begin{property}
  Let \(F\) be a partial tree \textit{(spanning nodes in \(S \subseteq N\))} contained in a optimal spanning tree of \(G = (N, E)\).
  Consider \(e = \left\{u, v\right\} \in \delta(S)\) of minimum cost, then there exists a minimum cost spanning tree of \(G\) containing \(e\).
\end{property}

\begin{proof}
  By contradiction, assume \(T^\ast \subseteq E\) is a minimum cost spanning tree with \(F \subseteq T^\ast\) and \(e \notin T^\ast\).

  Adding an edge \(e\) to \(T^\ast\) creates the cycle \(C\).
  Let \(f \in \delta(S) \cap C\):

  \begin{itemize}
    \item If \(c_e = c_f\), then \(T^\ast \cup \{e\}\setminus \{f\}\) is a minimum cost spanning tree of \(G\) as it has the same cost as \(T^\ast\)
    \item If \(c_e < c_f\), then \(c \left( T^\ast \cup \{e\} \setminus \{f\} \right) < c \left(T^\ast\right)\), hence \(T^\ast\) is not optimal
  \end{itemize}

\end{proof}

\newpage

\subsection{Optimal cost spanning tree}

\textbf{Spanning trees} have a number of applications, including:

\begin{itemize}
  \item \textbf{network} design
  \item \textbf{IP network} protocols
  \item \textbf{compact memory} storage
\end{itemize}

\subparagraph*{Model}
an undirected graph \(G = (N, E), \, n = |N|, \, m = |E|\) and a cost function \(c: E \rightarrow \R\), that assigns a cost to each edge, with \(e = \{u, v\} \in E\).

\subparagraph*{Required properties}
\begin{enumerate}
  \item Each \textbf{pair of nodes} must be in a \textbf{path}
        \begin{itemize}[label= \(\Rightarrow\)]
          \item the output must be a \textbf{connected subgraph} containing all the nodes \(N\) of \(G\)
        \end{itemize}
  \item The \textbf{subgraph} must have \textbf{no cycles}
        \begin{itemize}[label= \(\Rightarrow\)]
          \item the output must be a \textbf{tree}
        \end{itemize}
\end{enumerate}

\bigskip
\begin{problem}[Problem definition]
Given an undirected graph \(G = (N, E)\) and a cost function \(c: E \rightarrow \R\), find a spanning tree \(G_T(N, T)\) of \(G\) of minimum, total cost.

The objective is finding:
\[ \displaystyle \min_{T \in X} \sum_{e \in T} c_e \qquad X = \text{ set of all spanning trees of } G \]
\end{problem}

\begin{theorem}
  A complete graph with \(n\) nodes (\(n \geq 1\)) has \(n^{n-2}\) spanning trees.
\end{theorem}

\begin{property}
  Every spanning tree of a connected \(n\)-node graph has \(n-1\) edges.
\end{property}

\subsubsection{Prim's algorithm}

\textbf{Idea}: iteratively build a spanning tree.

\subparagraph*{Method}
\begin{enumerate}
  \item Start from initial tree \((S, T)\) with \(S = \{u\}, S \subseteq N\) and \(T = \emptyset\)
  \item At each step, add to the current partial tree \((S, T)\) an edge of minimum cost among those which connect a node in \(S\) to a node in \(N \setminus S\)
\end{enumerate}

\subparagraph*{Goal}
\begin{itemize}[label=\(\rightarrow\)]
  \item \textit{Input}: connected graph \(G = (N, E)\) with edge costs.
  \item \textit{Output}: subset \(T \subseteq N\) of edges of \(G\) such that \(G_T = (N, T)\) is a minimum cost spanning tree of \(G\).
\end{itemize}

\subparagraph*{Complexity}
if all edges are scanned at each iteration, the complexity order is \(\bigO{nm}\)

\subparagraph*{Algorithm}
the pseudocode of the algorithm is shown in Code~\ref{lst:prims-algorithm}.
Prim's algorithm is \textbf{greedy}:
at each step a minimum cost edge is selected among those in the cut \(\delta(S)\) induced by the current set of nodes \(S\).

\begin{lstlisting}[caption={Prim's algorithm}, label={lst:prims-algorithm}, float]
S := {u}
T := {}
while |T| < n - 1 do
  {u, v} := edge $\in \delta(\texttt{S})$ with minimum cost // $u \in S, v \in N \setminus S$
  S := S $\cup$ {v}
  T := T $\cup$ {{u, v}}
end
\end{lstlisting}

\paragraph{Correcteness of Prim's algorithm}

\begin{proposition}
  Prim's algorithm is exact.
\end{proposition}

The exactness does not depend on the choice of the first node nor on the selected edge of minimum cost \(\delta(S)\).
Each selected edge is part of the optimal solution as it belongs to a minimum spanning tree.

The optimality condition allows to verify whether a spanning tree \(T\) is optimal or not;
it suffices to check that each \(e \in E \setminus T\) is not a cost decreasing edge.

\paragraph{Implementation in quadratic time}

The Prim's algorithm can be implemented in quadratic time, i.e. \(\bigO{n^2}\).

\textbf{Data structure}
\begin{itemize}[itemsep=0.25ex]
  \item \(k\) number of edges selected so far
  \item Subset \(S \subseteq N\) of nodes incident to the selected edges
  \item Subset \(T \subseteq E\) of selected edges
  \item \(C_j = \begin{cases} \min \{c_{ij} \mid   i \in S\} \quad &j \notin S \\ +\infty &\text{otherwise}\end{cases}\)
  \item \(\textit{closest}_j = \begin{cases} \argmin\{c_{ij} \mid   i \in S\}  & j \notin S \\ \text{predecessor of } j \text{ in the minimum spanning tree} \quad& j \in S \end{cases}\)
\end{itemize}

\bigskip
An example of a step is shown in Figure~\ref{fig:prim-quadratic-data-structure}.

\begin{figure}[htbp]
  \centering
  \bigskip
  \centering
  \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
    \node[draw=red, text=red] (2) {2};
    \node[draw=red, text=red] (3) [right of = 2] {3};
    \node[draw=red, text=red] (1) [below of = 2] {1};
    \node[draw] (j) [below of = 3] {j};

    \path[-] (1) edge node[above] {\(4\)} (j);
    \path[-] (2) edge node[above] {\(5\)} (j);
    \path[-] (3) edge node[right] {\(1\)} (j);
  \end{tikzpicture}
  \caption{Data structure}
  \label{fig:prim-quadratic-data-structure}
  \shortstack{
    nodes \(1, 2, 3 \in S\), node \(j \notin S\) \\
    \(\text{closest}_j = 3 \quad c_{\text{closest}_j j} = 1\)
  }
  \bigskip
\end{figure}

The spanning tree is built is built by selecting the node \(j\) with minimum cost \(C_j\) and adding the edge \(\{j, \textit{closest}_j\}\) to the spanning tree.

\bigskip
The code for this algorithm is shown in Code~\ref{lst:prim-quadratic}.

\begin{lstlisting}[caption={Prim's algorithm in quadratic time}, label={lst:prim-quadratic}, float]
T := {}
S := {u}
// initialization
for j $\notin$ N $\setminus$ S do
  if {u, j} $\in$ E then
    C_j := c_{u, j}
  else
    C_j := +\infty
  end
  closest_j := u
end
for k := 1 to n - 1 do
  min := $+\infty$ // selection of min cost edge
  for j := 1, ..., n do
    if j $\notin$ S and C_j < min then
      min := C_j
      v := j
    end
  end
  S := S $\cup$ {v} // extend S
  T := T $\cup$ {{v, closest_v}} // extend T
  for j := 1 to n do
    if j $\notin$ S and c_vj < C_j then
      C_j := c_vj
      closest_j := v
    end
  end
end
\end{lstlisting}

\bigskip
The complexity of this algorithm is \(\bigO{n^2}\).
For sparse graphs, where \(m \ll \dfrac{n(n-1)}{2}\), a more efficient implementation (\(\bigO{m \log{(n)}}\)) \textit{(using priority queues)} is possible.

\subsubsection{Kruskal's algorithm}

Kruskal's algorithm has the same Goal of Prim's algorithm but with a different approach;
like the latter, is greedy.

\subparagraph*{Method}

\begin{enumerate}
  \item create a \textbf{forest} \(F\) where each vertex is a separate tree
  \item create a \textbf{set} \(S\) containing all the edges in the graph
  \item while \(S\) is \textbf{non empty }and \(F\) is not spanning:
        \begin{enumerate}
          \item \textbf{remove an edge} with minimum cost from \(S\)
          \item if the removed edge connects two different trees then \textbf{add it to the forest} \(F\), combining two trees into a single tree
        \end{enumerate}
\end{enumerate}

At the termination of the algorithm, the forest forms a minimum spanning forest of the graph.
If the graph is connected, the forest has a single component and forms a minimum spanning tree.

\subparagraph*{Complexity}
the complexity of the algorithm is \(\bigO{E \log}(E)\) with \(E\) the number of edges in the graph.

\subparagraph*{Algorithm}
the Kruskal's algorithm is implemented in Code~\ref{lst:kruskal}.

\begin{lstlisting}[caption={Kruskal's algorithm}, label={lst:kruskal}, float]
F := {}
S := {e_1, ..., e_m}, e $\in$ E, sorted by cost in non-decreasing order
i := 1
while |F| < n - 1 do
  if the two endpoints of e_i are in different trees then
    F := F $\cup$ {e_i}
    merge the two trees containing the two endpoints
  end
  i := i + 1
end15
\end{lstlisting}

\subsubsection{Optimality condition of a spanning tree}

Given a spanning tree \(T\), an edge \(e \notin T\) is \textbf{cost decreasing} if when added to \(T\), it creates a cycle \(C\) with \(C \subseteq T \cup \{e\}\) and \(\exists \, f \in C \setminus \{e\} \ \text{such that} \  c_e < c_f\).

\begin{theorem}
  A tree \(T\) is of minimum total cost if and only if no cost decreasing edge exists.
\end{theorem}

\begin{proof}
  \hfill
  \begin{itemize}
    \item[\(\Rightarrow\)] If a \textbf{cost decreasing edge exists}, then \(T\) is \textbf{not of minimum total cost}
    \item[\(\Leftarrow\)] If \textbf{no cost decreasing edge exists}, then \(T\) is \textbf{of minimum total cost}
      \begin{itemize}
        \item let \(T^\ast\) be a minimum cost spanning tree of graph \(G\), found via by Prim's algorithm
        \item it can be verified that \(T^\ast\) can be iteratively \textit{(changing one edge at a time)} transformed into \(T\) without changing the total cost
        \item thus, \(T\) is also optimal
      \end{itemize}
  \end{itemize}

\end{proof}

\subsection{Optimal paths}

\textbf{Optimal} \textit{(shortest, longest, \ldots)} paths have a wide range of applications, including:

\begin{itemize}
  \item \textbf{Google Maps}, \textbf{GPS} navigators
  \item Planning and management of \textbf{transportation}, \textbf{electrical}, and \textbf{telecommunication} \textbf{networks}
  \item \textbf{Problem} planning
\end{itemize}

\bigskip
\begin{problem}[Problem definition]
Given a directed graph \(G = (N, A)\) with a cost \(c_{ij} \in \R\) associated to each arc \((i, j) \in A\), and two nodes \(s\) and \(t\), determine a minimum cost \textit{(shortest)} path from \(s\) to \(t\).
\end{problem}

\begin{itemize}
  \item Each \textbf{value} \(c_{i, j}\) represents the \textbf{cost} \textit{(or length, travel time, \ldots)} of arc \((i, j) \in A\)
  \item Node \(s\) is the \textbf{origin} \textit{(or source)}, node \(t\) is the \textbf{destination} \textit{(or sink)}
\end{itemize}

\begin{property}
  A path \(\langle (i_1, i_2), (i_2, i_3), \, \ldots \,, (i_{k-1}, i_k)  \rangle\) is \textbf{simple} if no node is visited more than once
\end{property}

\begin{property}
  If \(c_{ij} \geq 0\) for all \((i, j) \in A\), there is at least one shortest path that is simple.
\end{property}

\subsubsection{Dijkstra's algorithm}

\textbf{Idea}: consider the nodes in increasing order of length \textit{(cost)} of the shortest path from \(s\) to any one of the other nodes.

\subparagraph*{Method}
\begin{itemize}
  \item To each \textbf{node} \(j \in N\), a \textbf{label} \(L_j\) is associated
        \begin{itemize}
          \item[\(\Rightarrow\)] at the end of the algorithm, this label will be the cost of the minimum cost path from \(s\) to \(j\)
        \end{itemize}
  \item Another label \(\textit{predecessor}_j\) is associated to each node \(j \in N\)
        \begin{itemize}
          \item[\(\Rightarrow\)] at the end of the algorithm, this label will be the node that precedes \(j\) on the minimum cost path from \(s\) to \(j\)
        \end{itemize}
  \item Make a \textbf{greedy} choice with respect to the paths from \(s\) to \(j\)
  \item A set of \textbf{shortest paths} from \(s\) to any node \(j \notin s\) can be retrieved backwards from \(t\) to \(s\) iterating over the predecessors
\end{itemize}

\subparagraph*{Goal}
\begin{itemize}[label=\(\rightarrow\)]
  \item \textit{Input}: graph \(G = (N, A)\), cost \(c_{ij} \geq 0 \, \forall \, i, j\), origin \(s \in N\)
  \item \textit{Output}: shortest path from \(s\) to all other nodes in \(G\)
\end{itemize}

\subparagraph*{Data structure}
\begin{itemize}
  \item \(S \subseteq N\): subset of nodes whose labels are permanent
  \item \(X \subseteq N\): subset of nodes with temporary labels
  \item \(L_j = \begin{cases} \text{cost of a shortest path from } s \text{ to } j \quad & j \in S \\ \min\{L_i + c_{ij} \mid   (i, j) \in \delta^+(S) & j \notin S\}\end{cases}\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item given a directed graph \(G\) and the current subset of nodes \(S \subset N\), consider the outgoing cut \(\delta^+(S)\) and select \((u, v) \in \delta^+(S)\) such that: \( L_u + c_{uv} = \min\{L_i + c_{ij} \mid   (i, j) \in \delta^+ (S)\}\)
          \item thus: \(L_u + c_{uv} \leq L_i + c_{ij}, \forall \, (i, j) \in \delta^+(S)\)
        \end{itemize}
  \item \(\text{predecessor}_j = \begin{cases}\text{predecessor of } j \text{ in the shortest path from } s \text{ to } j \quad& j \in S \\ u \text{ such that } L_u + c_{uj} = \min\{L_i + c_{ij} \mid   i \in S\} & j \notin S\end{cases}\)
\end{itemize}

\subparagraph*{Complexity}
the complexity of the algorithm depends on the how the arc \((u, v)\) is selected among those of the current cut \(\delta^+(u)\).
\begin{itemize}
  \item If all \(m\) arcs are scanned, the overall complexity would be \(\bigO{nm}\), hence \(\bigO{n^3}\)
  \item If all labels \(L_j\) are determined by appropriate updates (as in Prim's algorithm), only a single arc of \(\delta^+(j)\) is scanned, hence the complexity is \(\bigO{n^2}\)
\end{itemize}

\subparagraph*{Remarks}
\begin{itemize}
  \item A set of shortest paths from \(s\) to all the nodes \(j \in N\) can be retrieved backwards from \(t\) to \(s\) iterating over the predecessors
  \item The union of a set of shortest paths from node \(s\) to all the other nodes of \(G\) is an arborescence rooted at \(s\)
  \item Dijkstra's algorithm does not work when there are arcs with negative cost: if \(G\) contains a circuit of negative cost, the shortest path problem may not be well defined
\end{itemize}

\bigskip
The code for this algorithm is shown in Code~\ref{lst:dijkstras-algorithm}.

\begin{lstlisting}[caption={Dijkstra's algorithm}, label={lst:dijkstras-algorithm}, float]
S := {}
X := {s}
for u $\in$ N do
  L_u := $\infty$
end
L_s := 0
while |S| < |N| do
  u := argmin{L_i $|$ i $\in$ X}
  X := X $\backslash$ {u}
  S := S $\cup$ {u}
  for (u, v) $\in \delta^+(\texttt{u})$ do
    if L_v > L_u + c_uv then
      L_v := L_u + c_uv
      predecessor_v := u
      X := X $\cup$ {v}
    end
  end
end
\end{lstlisting}

\paragraph{Correcteness of Dijkstra's algorithm}

\begin{proposition}
  Dijkstra's algorithm is correct.
\end{proposition}

\begin{proof}
  \hfill
  \begin{enumerate}
    \item A the \(k\)-th step:
          \begin{itemize}
            \item \(S = \{s, i_1, \, \ldots \,, i_{k-1}\}\)
            \item \(\begin{cases}\text{cost of a minimum cost path from } s \text{ to } j & j \in S \\ \text{cost of a minimum cost path with all intermediate nodes in } S \quad & j \notin S\end{cases}\)
          \end{itemize}
    \item By induction on the number \(k\) of steps:
          \begin{itemize}
            \item base case: for \(k = 1\) the statement holds, since
                  \[S = \{s\}, \quad L_s = 0, \quad L_j = +\infty, \quad \forall \, j \notin S \]
            \item inductive step: assume that the statement holds for \(k+1\)
                  \begin{itemize}
                    \item let \(u \notin S\) be the node that is inserted in \(S\) and \(\phi\) the path from \(s\) to \(u\) such that:
                          \[ L_v + c_{vu} \leq L_i + c_{iu}, \quad \forall \, (i, v) \in \delta^+(S) \]
                    \item every path \(\pi\) from \(s\) to \(u\) has \(c(\pi) \geq c(\phi)\), as there exists \(i \in S\) and \(j \notin S\) such that:
                          \[ \pi = \pi_1 \cup \left\{ \left( i, j \right)  \right\} \cup \pi_2 \]
                          where \((i, j)\) is the first arc in \(\pi \cap \delta^+(S)\)
                    \item it holds that
                          \[ c(\pi) = c(\pi_1) + c_{ij} + c(\pi_2) \geq L_i + c_{ij} \]
                          because \(c_{ij} \geq 0 \Rightarrow c(\pi_2) \geq 0\) and by the choice of \((v, u)\), \(c(\pi_1) \geq L_i\)
                    \item finally, by induction assumption:
                          \[ L_i + c_{ij} \geq L_v + c_{vu} = c(\phi) \]
                    \item a visualization of this step of the proof is shown in Figure~\ref{fig:proof-of-the-induction-step}
                  \end{itemize}
          \end{itemize}
  \end{enumerate}
\end{proof}

\begin{figure}[htbp]
  \centering
  \bigskip
  \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
    \node[draw] (u) at (-2, 0) {u};
    \node[draw] (j) at (0, 0) {j};
    \node[draw=blue!60, text=white, fill=blue!30] (v) at (-2, -2) {v};
    \node[draw=blue!60, text=white, fill=blue!30] (i) at (0, -2) {i};
    \node[draw=blue!60, text=white, fill=blue!30] (s) at (-1, -4) {s};

    \draw[dashed, ->, draw=blue] (s) edge[bend left] node[left, text=blue] {\(\phi\)} (v);
    \draw[dashed, ->, draw=red] (s) edge[bend right] node[right, text=red] {\(\pi_1\)} (i);
    \draw[->, draw=red] (i) edge (j);
    \draw[->, draw=blue] (v) edge (u);
    \draw[dashed, ->, draw=red] (j) edge[bend right] node[above, text=red] {\(\pi_2\)} (u);
  \end{tikzpicture}
  \caption{Proof of the induction step; nodes \(s, v, i\) are in cut \(S\)}
  \label{fig:proof-of-the-induction-step}
  \bigskip
\end{figure}

\paragraph{Example of Dijkstra's algorithm}

An example of Dijkstra's algorithm is shown in Figure~\ref{fig:dijkstra-example}.

\begin{figure}[tbp]
  \centering
  \bigskip
  \begin{subfigure}[t]{\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \samplegraph

      \path[->] (1) edge node[left=-0.5mm] {5} (2);
      \path[->] (1) edge node[below=-0.5mm] {9} (4);
      \path[->] (2) edge[bend left=15] node[above=-1mm] {3} (3);
      \path[->] (2) edge node[left=-0.5mm] {2} (4);
      \path[->] (3) edge[bend left=15] node[below=-1mm] {1} (2);
      \path[->] (3) edge[bend left=15] node[right=-1mm] {0} (4);
      \path[->] (4) edge[bend left=15] node[left=-1mm] {2} (3);
      \path[->] (3) edge node[above=-0.5mm] {5} (5);
      \path[->] (4) edge node[below=-0.5mm] {9} (5);
    \end{tikzpicture}
    \caption{Sample graph, with the cost of each arc}
  \end{subfigure}
  \bigskip
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \node[draw, label={[text=gray, above=-0.4cm]\((\infty, -)\)}] (2) {2};
      \node[draw, label={[text=gray, above=-0.4cm]\((\infty, -)\)}] (3) [right of = 2] {3};
      \node[draw, label={[text=gray, above=-0.4cm]\((\infty, -)\)}] (5) [right of = 3] {5};
      \node[draw=red, text=red, label={[text=red, below=0.5cm]\((0, -)\)}] (1) [below of = 2] {1};
      \node[draw, label={[text=gray, below=0.4cm]\((\infty, -)\)}] (4) [right of = 1] {4};
    \end{tikzpicture}
    \caption{step 1 of Dijkstra's algorithm}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \node[draw=red, text=red, label={[text=red, above=-0.4cm]\((5, 1)\)}] (2) {2};
      \node[draw, label={[text=gray, above=-0.5cm]\((\infty, -)\)}] (3) [right of = 2] {3};
      \node[draw, label={[text=gray, above=-0.5cm]\((\infty, -)\)}] (5) [right of = 3] {5};
      \node[draw=blue, text=blue, label={[text=blue, below=0.5cm]\((0, -)\)}] (1) [below of = 2] {1};
      \node[draw=red, text=red, label={[text=red, below=0.5cm]\((9, 1)\)}] (4) [right of = 1] {4};

      \path[->] (1) edge node[left=-0.5mm] {5} (2);
      \path[->] (1) edge node[below=-0.5mm] {9} (4);
    \end{tikzpicture}
    \caption{step 2 of Dijkstra's algorithm}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \node[draw=blue, text=blue, label={[text=blue, above=-0.4cm]\((5, 1)\)}] (2) {2};
      \node[draw=red, text=red, label={[text=red, above=-0.4cm]\((8, 2)\)}] (3) [right of = 2] {3};
      \node[draw, label={[text=gray, above=-0.5cm]\((\infty, -)\)}] (5) [right of = 3] {5};
      \node[draw=blue, text=blue, label={[text=blue, below = 0.5]\((0, -)\)}] (1) [below of = 2] {1};
      \node[draw=red, text=red, label={[text=red, below = 0.5]\((7, 2)\)}] (4) [right of = 1] {4};

      \path[->, blue] (1) edge node[left=-0.5mm] {5} (2);
      \path[->] (1) edge node[below=-0.5mm] {9} (4);
      \path[->] (2) edge[bend left=15] node[above=-1mm] {3} (3);
      \path[->] (2) edge node[left=-0.5mm] {2} (4);
    \end{tikzpicture}
    \caption{step 3 of Dijkstra's algorithm}
  \end{subfigure}
  \bigskip
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \node[draw=blue, text=blue, label={[text=blue, above=-0.4cm]\((5, 1)\)}] (2) {2};
      \node[draw=red, text=red, label={[text=red, above=-0.4cm]\((8, 2)\)}] (3) [right of = 2] {3};
      \node[draw=red, text=red, label={[text=red, above=-0.4cm]\((16, 4)\)}] (5) [right of = 3] {5};
      \node[draw=blue, text=blue, label={[text=blue, below=0.4cm]\((0, -)\)}] (1) [below of = 2] {1};
      \node[draw=blue, text=blue, label={[text=blue, below=0.4cm]\((7, 2)\)}] (4) [right of = 1] {4};

      \path[->, blue] (1) edge node[left=-0.5mm] {5} (2);
      \path[->] (2) edge[bend left=15] node[above=-1mm] {3} (3);
      \path[->, blue] (2) edge node[left=-0.5mm] {2} (4);
      \path[->] (4) edge[bend left=15] node[left=-1mm] {2} (3);
      \path[->] (4) edge node[below=-0.5mm] {9} (5);
    \end{tikzpicture}
    \caption{step 4 of Dijkstra's algorithm}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \node[draw=blue, text=blue, label={[text=blue, above=-0.4cm]\((5, 1)\)}] (2) {2};
      \node[draw=blue, text=blue, label={[text=blue, above=-0.4cm]\((8, 2)\)}] (3) [right of = 2] {3};
      \node[draw=red, text=red, label={[text=red, above=-0.4cm]\((13, 3)\)}] (5) [right of = 3] {5};
      \node[draw=blue, text=blue, label={[text=blue, below=0.4cm]\((0, -)\)}] (1) [below of = 2] {1};
      \node[draw=blue, text=blue, label={[text=blue, below=0.4cm]\((7, 2)\)}] (4) [right of = 1] {4};

      \path[->, blue] (1) edge node[left=-0.5mm] {5} (2);
      \path[->, blue] (2) edge[bend left=15] node[above=-1mm] {3} (3);
      \path[->, blue] (2) edge node[left=-0.5mm] {2} (4);
      \path[->] (3) edge[bend left=15] node[below=-1mm] {1} (2);
      \path[->] (3) edge node[above=-0.5mm] {5} (5);
      \path[->] (4) edge node[below=-0.5mm] {9} (5);
    \end{tikzpicture}
    \caption{step 5 of Dijkstra's algorithm}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
      \node[draw=blue, text=blue, label={[text=blue, above=-0.4cm]\((5, 1)\)}] (2) {2};
      \node[draw=blue, text=blue, label={[text=blue, above=-0.4cm]\((8, 2)\)}] (3) [right of = 2] {3};
      \node[draw=blue, text=blue, label={[text=blue, above=-0.4cm]\((13, 3)\)}] (5) [right of = 3] {5};
      \node[draw=blue, text=blue, label={[text=blue, below=0.4cm]\((0, -)\)}] (1) [below of = 2] {1};
      \node[draw=blue, text=blue, label={[text=blue, below=0.4cm]\((7, 2)\)}] (4) [right of = 1] {4};

      \path[->, blue] (1) edge node[left=-0.5mm] {5} (2);
      \path[->, blue] (2) edge[bend left=15] node[above=-1mm] {3} (3);
      \path[->, blue] (2) edge node[left=-0.5mm] {2} (4);
      \path[->, blue] (3) edge node[above=-0.5mm] {5} (5);
    \end{tikzpicture}
    \caption{step 6 of Dijkstra's algorithm}
  \end{subfigure}
  \caption{Example of Dijkstra's algorithm}
  \label{fig:dijkstra-example}
\end{figure}

\subsubsection{Floyd-Warshall's algorithm}

\subparagraph*{Goal}
\begin{itemize}[label=\(\rightarrow\)]
  \item \textit{Input}: a directed graph \(G = (N, A)\) with an \(n \times n\) cost matrix \(C = [c_{ij}]\)
  \item \textit{Output}: for each pair of nodes \(i, j \in N\), the cost \(c_{ij}\) of the shortest path from \(i\) to \(j\)
\end{itemize}

\subparagraph*{Data structure}
\begin{itemize}
  \item Two \(n \times n\) matrices \(D, P\) whose elements correspond, at the end of the algorithm, to:
        \begin{itemize}
          \item \(d_{ij}\) the cost of the shortest path from \(i\) to \(j\)
          \item \(p_{ij}\) the predecessor of \(j\) on the shortest path from \(i\) to \(j\)
        \end{itemize}
\end{itemize}

\subparagraph*{Method}
\begin{enumerate}
  \item \textbf{Initialization} of \(D\) and \(P\):
        \begin{gather*}
          p_{ij} = i \quad \forall \, i \\
          d_{ij} = \begin{cases} 0 \quad & i = j \\ c_{ij} & i \neq j \land (i, j) \in A \\ +\infty & \text{otherwise}  \end{cases}
        \end{gather*}

  \item \textbf{Triangular} operation: for each pair of nodes \(i, j\), where \(i \neq u, j \neq u\), check whether the path from \(i\) to \(j\) is shorter by going through \(u\) (i.e. \(d_{iu} + d_{uj} < d_{ij}\))
\end{enumerate}

\subparagraph*{Complexity}
\begin{itemize}
  \item Since in the worst case the triangular operation is executed for all nodes \(u\) ad for each pair of nodes \(i, j\), the complexity is \(\bigO{n^3}\)
\end{itemize}

\bigskip
The code for this algorithm is shown in Code~\ref{lst:floyd-warshall-algorithm}.

\begin{lstlisting}[caption={Floyd-Warshall's algorithm}, label={lst:floyd-warshall-algorithm}, float]
for j := 1 to n do
  p_id := i
  if i = j then
    d_ij := 0
  else if (i, j) in A then
    d_ij := c_ij
  else
    d_ij := $+\infty$
  end
end
for u $\in$ N do
  for i $\in$ N $\setminus\{$ u $\}$ do
    for j $\in$ N $\setminus\{$ u $\}$
      if d_iu + d_uj < d_ij then
        p_ij := p_uj
        d_ij := d_iu + d_uj
      end
    end
  for i $\in$ N do
    if d_ij < 0 then
      error "negative cycle"
    end
  end
end
\end{lstlisting}

\paragraph{Correctness of Floyd-Warshall's algorithm}

\begin{proposition}
  Floyd-Warshall's algorithm is correct.
\end{proposition}

\begin{proof}
  assume that the nodes of \(G\) are numbered from \(1\) to \(n\).
  Verify that, if the node index order is followed, after the \(u\)-th cycle the value \(d_{ij}\) \textit{(for any \(i, j\))} corresponds to the cost of a shortest path from \(i\) to \(j\) with at most \(u\) intermediate nodes (\(\left\{ 1, \, \ldots \,, u \right\}\))
\end{proof}

\subsection{Optimal paths in directed, acyclic graphs}

A directed graph \(G = (N, A)\) is \textbf{acyclic} if it does not contain any circuit.
A directed acyclic graph \(G\) is referred to as a \DAG.

\begin{property}[Topological ordering]
  The nodes of any \DAG \(G\) can be ordered topologically, i.e. indexed so that for each arc \((i, j) \in A\) the index of \(i\) is less than the index of \(j\) \textit{(\(i \leq j\))}.
\end{property}

The topological order can be exploited by dynamic programming algorithms to compute efficiently the shortest paths in a \DAG.

\begin{problem}[Problem description]
Given a \DAG \(G = (N, A)\) with a cost \(c_{ij} \in \R\) and nodes \(s, t\), determine the shortest \textit{(or longest)} path from \(s\) to \(t\).
\end{problem}

\subsubsection{Topological ordering method}

The method requires \(G = (N, A)\) to be a \DAG represented via the list of predecessors \(\delta^-(v)\) and the list of successors \(\delta^+(v)\) of each node \(v \in N\).
Then, it works as follows:

\begin{enumerate}[label=\arabic*., ref=(\arabic*)]
  \item\label{enum:topological-ordering-1} \textbf{Assign} the smallest positive integer not yet assigned to a node \(v \in N \) with \(\delta^-(v) = \emptyset\)
  \begin{itemize}[label=\(\rightarrow\)]
    \item such node always exists because \(G\) does not contain circuits
  \end{itemize}
  \item \textbf{Delete} the node \(v\) with all its incident arcs
  \item \textbf{Go} to step  \ref{enum:topological-ordering-1} until all nodes have been assigned a number
\end{enumerate}

\begin{figure}[htbp]
  \centering
  \bigskip
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[circle]
      \node[draw, circle, minimum size=0.75cm] (v) at (0, 0) {\(v\)};
      \node[draw=gray,circle, minimum size=0.5cm] (0) at (-1, -1) {};
      \node[draw=gray,circle, minimum size=0.5cm] (1) at (-2, 0) {};
      \node[draw=gray,circle, minimum size=0.5cm] (2) at (-1, 1) {};
      \node[draw=gray,circle, minimum size=0.5cm] (3) at (1, -1) {};
      \node[draw=gray,circle, minimum size=0.5cm] (4) at (2, 0) {};
      \node[draw=gray,circle, minimum size=0.5cm] (5) at (1, 1) {};

      \draw[->] (0) -- (v);
      \draw[->] (1) -- (v);
      \draw[->] (2) -- (v);

      \draw[->] (v) -- (3);
      \draw[->] (v) -- (4);
      \draw[->] (v) -- (5);
    \end{tikzpicture}
    \caption{Topological ordering}
    \label{fig:topological-ordering}
  \end{subfigure}
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[circle]
      \node[draw, circle, minimum size=0.75cm] (v) at (0, 0) {\(v\)};
      \node[draw=gray,circle, minimum size=0.5cm] (0) at (-1, -1) {};
      \node[draw=gray,circle, minimum size=0.5cm] (1) at (-2, 0) {};
      \node[draw=gray,circle, minimum size=0.5cm] (2) at (-1, 1) {};

      \draw[->] (0) -- (v);
      \draw[->] (1) -- (v);
      \draw[->] (2) -- (v);
    \end{tikzpicture}
    \caption{Node \(v\) with \(\delta^-(v) = \emptyset\), as in step \ref{enum:topological-ordering-1} of the algorithm}
    \label{fig:node-with-empty-successors}
  \end{subfigure}
  \caption{Topological ordering method}
  \label{fig:topological-ordering-method}
  \bigskip
\end{figure}

This algorithm has complexity \(\bigO{|A|}\), because each node is assigned a number only once.
Furthermore, all arcs incident to a node are deleted only once.

\subsubsection{Dynamic programming for shortest path in \DAGs}

Any \textbf{shortest path} from \(1\) to \(t\), called \(\pi_t\), with at least \(2\) arcs can be subdivided into two parts:

\begin{itemize}
  \item \(\pi_i\), the shortest \textbf{subpath} from \(s\) to \(i\)
  \item \(\left( i, t \right)\), the \textbf{remaining part}
\end{itemize}

This decomposition is called the \textbf{optimality principle of shortest paths in \DAGs}.
An illustration of this decomposition is shown in Figure \ref{fig:shortest-path-in-DAG}.

\begin{figure}[htbp]
  \centering
  \bigskip
  \begin{tikzpicture}[circle, minimum size = 0.75cm]
    \node[draw, circle] (1) at (-1, 0) {\(1\)};
    \node[draw, circle] (i) at (3, 0) {\(i\)};
    \node[draw, circle] (t) at (5, 0) {\(t\)};

    \draw[] (1) edge[dashed, ->] node[above=-0.2cm] {\(\pi_i\)} (i);
    \draw[->] (i) edge node[above=-0.2cm] {\(c_{it}\)} (t);
  \end{tikzpicture}
  \caption{Shortest path from \(1\) to \(t\)}
  \label{fig:shortest-path-in-DAG}
  \bigskip
\end{figure}

\bigskip
The strategy to find the shortest path is:

\begin{enumerate}
  \item For each node \(i = 1, \, \ldots \,, t\) let \(L_i\) be the cost of a shortest path from \(1\) to \(i\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(L_t = \displaystyle \min_{(i, t) \in \delta^-(t)} \left\{ L_i + c_{it} \right\}\)
          \item the minimum is taken over all possible predecessors \(i\) of \(t\)
        \end{itemize}
  \item If \(G\) is topologically ordered \DAG, then the only possible predecessors of \(t\) in a shortest path \(\pi_t\) from \(1\) to \(t\) are those with index \(i < t\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(L_t = \displaystyle \min_{i < t} \left\{ L_i + c_{it} \right\}\)
          \item in a graph with circuits, any node \(i\) can be a predecessor of \(t\) if \(i \neq t\)
        \end{itemize}
\end{enumerate}

For \DAGs whose nodes are topologically ordered \(L_{t-1}, \, \ldots \,, L_1\) satisfy the same type of recursive relations:

\[ L_{t-1} = \displaystyle \min_{i < t-1} \left\{ L_i + c_{i, t-1} \right\}; \, \ldots \,; L_2 = \displaystyle \min_{i=1}\left\{ L_i + c_{i2} \right\} = L_1 + c_{12}; L_1 = 0 \]
which can be solved in reversed order
\[ L_1 = 0; L_2 = L_1 + c_{12}; \, \ldots \,; L_{t} = \displaystyle \min_{i < t - 1} \left\{ L_i + c_t \right\} \]

\subparagraph*{Algorithm}
finally, the algorithm is shown in pseudocode in Code~\ref{lst:shortest-path-in-DAG}.

\begin{lstlisting}[caption={Shortest path in \DAG}, label={lst:shortest-path-in-DAG}, float]
sort the nodes of G topologically
L_1 := 0
for j := 2 to n do
  L_j := min{L_i + c_{ij} | (i, j) $\in \delta^-(\texttt{j})\ \land\ \texttt{i} < \texttt{j}$}
  pred_j := v such that (v, j) = argmin{L_i + c_{ij} | (i, j) $\in \delta^-(\texttt{j})\ \land\ \texttt{i} < \texttt{j}$}
end
\end{lstlisting}

\subparagraph*{Complexity}
of the algorithm is \(\bigO{|A|}\):

\begin{itemize}
  \item Topological ordering of the nodes: \(\bigO{m}\) with \(m = |A|\) \textit{(number of arcs)}
  \item Each node/arc is processed only once: \(\bigO{n+m}\)
\end{itemize}

\bigskip
In order to find the longest path, the algorithm can be adapted as follows:
\[L_t = \displaystyle \max_{i < t} \left\{ L_i + c_{it} \right\}\]

\paragraph{Optimality of the algorithm}

The Dynamic Programming algorithm for finding shortest or longest paths in \DAGs is exact.
This is due to the optimality principle, already explored in the previous section.

\subsection{Project planning}

\begin{problem}[Problem definition]
A project consists of a set of \(m\) activities with their (estimated) duration: activity \(A_i\) has duration \(d_i \geq 0, i = 1, \, \ldots \,, m\).
Some pair of activities allow a precedent constraint: \(A_i \propto A_j\) indicated that \(A_i\) must be performed before \(A_j\).

A project can be represented by a directed graph \(G = (N, A)\) where:
\begin{itemize}
  \item each \textbf{arc} corresponds to an \textbf{activity}
  \item the \textbf{arc length} represent the \textbf{duration} of the corresponding activity
\end{itemize}
\end{problem}

\bigskip
In order to account for precedence constraints, the arcs must be positioned such that for activities \(A_i \propto A_j\) there exists a directed path where the arc associated to \(A_i\) precedes the arc associated to \(A_j\).
Such notation is shown in Figure \ref{fig:precedence-relation-in-project-planning}.

\begin{figure}[htbp]
  \centering
  \bigskip
  \begin{tikzpicture}[circle, node distance = 2cm, minimum size = 0.75cm]
    \node[draw] (1) {};
    \node[draw] (2) [right of = 1] {};
    \node[draw] (3) [right of = 2] {};
    \node[draw] (4) [right of = 3] {};
    \node[draw] (5) [right of = 4] {};
    \node[draw] (6) [right of = 5] {};

    \draw[dashed, ->] (1) edge (2);
    \draw[->] (2) edge node[above=-0.2cm]{\(A_i\)} (3);
    \draw[dashed, ->] (3) edge (4);
    \draw[->] (4) edge node[above=-0.2cm]{\(A_j\)} (5);
    \draw[dashed, ->] (5) edge (6);
  \end{tikzpicture}
  \bigskip
  \caption{Precedence relation in project planning}
  \label{fig:precedence-relation-in-project-planning}
\end{figure}

Therefore, a nove \(v\) marks an event corresponding to the end fo all the activities \(\left( i, v \right) \in \delta^-(v)\) and the \textit{(possible)} start of all the activities \(\left( v, j \right) \in \delta^+(v)\).

\bigskip
\begin{property}
  The directed graph \(G\) representing a project is acyclic \textit{(is a \DAG)}.
\end{property}

\begin{proof}
  by contradiction, if \(A_{i1} \propto A_{12} \propto \, \ldots \, \propto A_{jk} \propto A_{kj}\) there would be a logical inconsistency.
\end{proof}

\subsubsection{Optimal paths}

A graph \(G\) can be simplified by contracting some arcs, but it's important to not introduce unwanted precedence constraints.
Artificial nodes or artificial arcs are introduced so that graph \(G\):

\begin{itemize}
  \item Contains a \textbf{unique initial node} \(s\) corresponding to the event \inlinequote{beginning of the project}
  \item Contains a \textbf{unique final node} \(t\) corresponding to the event \inlinequote{end of the project}
  \item Does not contain \textbf{multiple arcs} with the \textbf{same origin and destination}
\end{itemize}

\bigskip
\begin{problem}[Problem description]
Given a project \textit{(set of activities with duration and precedence constraints)}, schedule the activities in order to minimize the overall project duration \textit{(the time needed to complete all the activities)}.
\end{problem}

\bigskip
\begin{property}
  The minimum overall project duration is the length of a longest path from \(s\) to \(t\) in the graph \(G\).
\end{property}

\begin{proof}
  since any \(s-t\) path represents a sequence of activities that must be executed in the specified order, its length provides a lower bound on the minimum overall project duration.
\end{proof}

\paragraph{Critical path method - \CPM}

The critical path method \textit{(\CPM) } determines:

\begin{itemize}
  \item A \textbf{schedule} \textit{(a plan for executing the activities specifying the order and the assigned time)} that minimizes the overall project duration
  \item The \textbf{slack} of each activity \textit{(the amount of time by which its execution can be delayed without affecting the overall minimum project duration)}
\end{itemize}

\subparagraph*{Initialization}
construct the graph \(G\) representing the project.

\subparagraph*{Method}

\begin{enumerate}
  \item Find a topological \textbf{order of the nodes}
  \item Consider the nodes by \textbf{increasing indices} and for each \(h \in N\) find the earliest time \(T_{min_h}\) at which the event associated to node \(h\) can occur
        \begin{itemize}[label = \(\rightarrow\)]
          \item \(T_{min_h}\) corresponds to the minimum project duration
        \end{itemize}
  \item Consider the nodes by \textbf{decreasing indices} and for each \(h \in N\) find the latest time \(T_{max_h}\) at which the event associated to node \(h\) can occur without delaying the project completion date beyond \(T_{min_n}\)
  \item For each activity \(\left( i, j \right) \in A\) find the \textbf{slack}
        \begin{itemize}[label = \(\rightarrow\)]
          \item the slack is calculated as \(\sigma{ij} = T_{max_j} - T_{min_i} - d_{ij}\)
        \end{itemize}
\end{enumerate}

\subparagraph*{Goal}
\begin{itemize}[label=\(\rightarrow\)]
  \item \textit{Input:} graph \(G = (N, A)\) with \(n = |N|\) and the duration \(d_{ij}\) associated to each \(\left( i, j \right) \in A\)
  \item \textit{Output}: \(\left( T_{min_i}, T_{max_i} \right), i = 1, \, \ldots \,, n\)
\end{itemize}

\subparagraph*{Algorithm}
finally, the algorithm is shown in pseudocode in Code~\ref{lst:critical-path-method}.

\begin{lstlisting}[caption={Critical path method}, label={lst:critical-path-method}, float]
sort the nodes topologically
T_min_i := 0
for j = 2 to n do
  T_min_j := max{T_min_i + d_ij | (i, j) $\in \delta^- (\texttt{j})$}
end
T_max_n := T_min_n // minimum project duration
for i = n-1 to 1 do
  T_max_i := min{T_max_j - d_ij | (i, j) $\in \delta^+ (\texttt{i})$}
end
\end{lstlisting}

\subparagraph*{Complexity}
the overall complexity is \(\bigO{n+m} \approx \bigO{m}\), due to the sum of:

\begin{itemize}
  \item complexity of the topological sort - \(\bigO{n+m}\)
  \item complexity of the first loop - \(\bigO{n+m}\)
  \item complexity of the second loop - \(\bigO{n+m}\)
\end{itemize}

\paragraph{Critical paths}

An activity \(\left( i, j \right)\) with zero slack \(\sigma_{ij} = T_{max_j} = T_{min_i} = d_{ij} = 0\) is called \textbf{critical}.

A critical path is a path in a \(s-t\) composed uniquely by critical activities.
At least one always exists.

\paragraph{Gantt charts}

A \textbf{Gantt chart} is a graphical representation of a project schedule.
It was introduced in \textit{1896} by \textit{Henry Gantt}, an American mechanical engineer and management consultant.

There are two types of Gantt charts:

\begin{itemize}
  \item Gantt chart at \textbf{earliest} - each activity \(\left( i, j \right)\) starts at \(T_{min_i}\) and ends at \(T_{min_i} + d_{ij}\)
  \item Gantt chart at \textbf{latest} - each activity \(\left( i, j \right)\) starts at \(T_{max_i}\) and ends at \(T_{max_i} + d_{ij}\)
\end{itemize}

\subsection{Network flows}

Network flows problems involve the distribution of a given \textit{product} \textit{(such as water, gas, data)} from a set of \textit{sources} to a set of \textit{users} so as to optimize a given objective function \textit{(e.g. minimize the total cost of the distribution)}.

It has many indirect applications, such as:

\begin{itemize}
  \item \textbf{Telecommunication}
  \item \textbf{Transportation}
  \item \textbf{Logistics}
\end{itemize}

\subparagraph*{Definitions
}
\begin{itemize}
  \item A \textbf{network} is a directed and connected graph \(G = (V, A)\) with a source \(s \in V\) and a sing \(t \in V\), with \(s \neq t\), and a capacity \(k_{ij} \geq 0\) for each arc \((i, j) \in A\).
  \item A \textbf{feasible flow} \(x\) from \(s\) to \(t\) is a vector \(x \in \R^m\) with a component \(x_{ij}\) for each arc \((i, j) \in A\) satisfying the capacity constraint \[0 \leq x_{ij} \leq k_{ij}, \quad \forall \, \left( i, j \right) \in A\]
        and the flow balance constraint at each intermediate node \(u \in \left\{ V \setminus \{s, t\} \right\}\):
        \[\displaystyle \sum_{\left( i, u \right) \in \delta^{-}(u)} x_{iu} = \displaystyle \sum_{\left( u, j \right) \in \delta^{+}(u)} x_{uj} \quad \forall \, u \in N \setminus \left\{ s, t \right\} \]
  \item The value of \textbf{flow} \(x\) is
        \[\phi = \displaystyle \sum_{\left( s, j \right) \in \delta^{+}(s)} x_{sj}\]
  \item Given a network and a feasible flow \(x\), an arc \(\left( i, j \right) \in A\) is \textbf{saturated} if \(x_{ij} = k_{ij}\) and \textbf{empty} if \(x_{ij} = 0\)
\end{itemize}

The notation of the flow network graph is shown in Figure~\ref{fig:flow-graph-representation};
a problem related to flow is defined as follows \textit{(Definition~\ref{def:flow-problem})}.

\begin{definition}[Flow Problem]
  Given a network \(G = \left( V, A \right)\) with an integer capacity \(k_{ij}\) for each arc \((i, j) \in A\), find a feasible flow \(x\) from \(s\) to \(t\) with maximum value.

  If multiple sources \textit{(\(s_1, s_2, \, \ldots \))} or sinks \textit{(\(t_1, t_2, \, \ldots \))} are present while only one product is considered, dummy notes \(s^\ast\) and \(t^\ast\) can be added \textit{(as shown in Figure~\ref{fig:multiple-sources-sinks})}.
  \label{def:flow-problem}
\end{definition}

\begin{figure}[htbp]
  \centering
  \bigskip
  \begin{tikzpicture}[circle, minimum size = 0.75cm]
    \node[draw] (s) at (0, 0) {\(s\)};
    \node[draw] (i) at (2, 0) {\(i\)};
    \node[draw] (j) at (5, 0) {\(j\)};
    \node[draw] (t) at (7, 0) {\(t\)};

    \path[->, dashed] (s) edge (i);
    \path[->] (i) edge node[above] {\(x_{ij}, k_{ij}\)} (j);
    \path[->, dashed] (j) edge (t);
  \end{tikzpicture}
  \caption{Graph representation of a flow}
  \label{fig:flow-graph-representation}
  \bigskip
\end{figure}
\begin{figure}[htbp]
  \centering
  \bigskip
  \begin{tikzpicture}[circle, minimum size = 0.75cm]
    \node[draw] (s) at (0, 0) {\(s^\ast\)};
    \node[draw] (s1) at (2, 1.41) {\(s_1\)};
    \node[draw] (s2) at (2, 0) {\(s_2\)};
    \node[] (s3) at (2, -1.41) {\vdots};

    \node[draw] (t1)  at (4, 1.41) {\(t_1\)};
    \node[draw] (t2)  at (4, 0) {\(t_2\)};
    \node[] (t3)  at (4, -1.41) {\vdots};
    \node[draw] (t) at (6, 0) {\(t^\ast\)};

    \path[->] (s) edge (s1);
    \path[->] (s) edge (s2);
    \path[->] (s) edge (s3);

    \path[->] (t1) edge (t);
    \path[->] (t2) edge (t);
    \path[->] (t3) edge (t);
  \end{tikzpicture}
  \caption{Multiple sources and sinks. \(\delta^-(s^\ast) = \delta^+(t^\ast) = \emptyset\), \(k_{s^\ast i} = \text{availability limit}\), \(k_{jt^\ast} = \infty\)}
  \label{fig:multiple-sources-sinks}
  \bigskip
\end{figure}


\subsubsection{Linear programming model}

The linear programming model \textit{(refer to Section~\ref{sec:linear-programming} for a in dept explanation of linear programming)} of the network flow is shown in \textit{Definition~\ref{def:network-flow-lp-model}}.

\begin{definition}[Linear Programming Model of Network Flow]
  \label{def:network-flow-lp-model}

  The Network Flow problem can be modelled as a linear programming problem as follows:

  \begin{gather*}
    \begin{aligned}
      \max \quad        & \phi                                                                                                                                                                                       \\
      \text{s.t.} \quad & \displaystyle \sum_{\left( u, j \right) \in \delta^+(u)} x_{uj} + \displaystyle \sum_{\left( i, u \right) \in \delta^-(u)} x_{iu} & = \begin{cases}
                                                                                                                                                                  \phi \quad   & \text{if } u = s \\
                                                                                                                                                                  - \phi \quad & \text{if } u = t \\
                                                                                                                                                                  0 \quad      & \text{otherwise}
                                                                                                                                                                \end{cases} \\
                        & 0 \leq x_{ij} \leq k_{ij} \quad \forall \, \left( i, j \right) \in A                                                                                                                       \\
                        & \phi \in \R, \quad x_{ij} \in \R \quad \forall \, \left( i, j \right) \in A
    \end{aligned}
  \end{gather*}

  where \(\phi\) denotes the value of the \textbf{feasible flow} \(x\);
  \(\phi\) is also the amount of product extracted from the source \(s\).
\end{definition}

\begin{itemize}
  \item A \textbf{cut} separating \(s\) from \(t\) is \(\delta(S) \text{ of } G, \ s \in S, \ t \in V \setminus S\)
  \item There are \(2^{n-2}\) cuts from \(s\) to \(t\), with \(n = |V|\)
  \item The \textbf{capacity} of the cut \(\delta(S)\) induced by \(S\) is: \[ k(S) = \displaystyle \sum_{\left( i, j \right) \in \delta^+(S)} k_{ij} \]
  \item Given a feasible flow \(x\) from \(s\) to \(t\) and a cut \(\delta(S)\) separating them, the value of the \textbf{feasible flow through the cut} is \[\phi(S) = \displaystyle \sum_{\left( i, j \right) \in \delta^+(S)} x_{ij} - \sum_{\left( i, j \right) \in \delta^-(S)} x_{ij}\]
        with this notation, the value of the flow \(x\) is \(\phi = \phi\left( \left\{ s \right\} \right)\)
\end{itemize}

\subsubsection{Properties of the flows}

\begin{property}
  Given a feasible flow \(x\) from \(s\) to \(t\), for each cut separating \(\delta(S)\) separating \(s\) from \(t\):
  \[ \phi(S) = \phi\left( \left\{ s \right\} \right) \]

  This Property is implied by the flow balance equations \(\forall \, v \ \in V \setminus \left\{ s, t \right\}\).
\end{property}

\begin{property}[Weak duality]
  \label{prop:weak-duality}
  For every feasible flow \(x\) from \(s\) to \(t\) and every cut \(\delta(S), \ S \subseteq V\) separating \(s\) from \(t\):
  \[ \phi(s) \leq k(S) \quad \left( \text{value of the flow} \ \leq \ \text{capacity of the cut} \right)\]
\end{property}

\begin{proof}
  By definition of value of the flow through the cut \(\delta(S)\):
  \[ \phi(S) = \displaystyle \sum_{\left( i, j \right) \in \delta^+(S)} x_{ij} - \sum_{\left( i, j \right) \in \delta^-(S)} x_{ij} \]
  And, since \(0 \leq x_{ij} \leq k_{ij}\) for all \(\left( i, j \right) \in A\):
  \[ \displaystyle \sum_{\left( i, j \right) \in \delta^+(S)} x_{ij} - \sum_{\left( i, j \right) \in \delta^-(S)} x_{ij} \leq \displaystyle \sum_{\left( i, j \right) \in \delta^+(S)} k_{ij} = k(s) \]
  Finally, \(\phi(S) \leq k(S)\).
\end{proof}

\subparagraph*{Consequence}
If \(\phi(S) = k(S)\), for a subset \(S \subseteq V, \ s \in S, \ t \notin S\), then \(x\) is a flow of maximum value and the cut \(\delta(S)\) is of minimum capacity.

The property \textit{(\ref{prop:weak-duality})} for any feasible flow \(x\) and for any cut \(\delta(S)\) separating \(s\) from \(t\) expresses a \textbf{weak duality relation} between the two problems of finding a maximum flow and a minimum cut.

\begin{itemize}
  \item \textbf{Primal problem}: given \(G = (V, A)\) with integer capacities on the arcs and \(s, t \in V\), determine a feasible flow of maximum problem
  \item \textbf{Dual problem}: given \(G = (V, A)\) with integer capacities on the arcs and \(s, t \in V\), determine a cut \textit{(separating \(s\) from \(t\))} of minimum capacity separating \(s\) from \(t\)
\end{itemize}

\subsubsection{Ford-Fulkerson algorithm}

Before presenting the Ford-Fulkerson algorithm, it's necessary to introduce the concept of Forward and Backward arcs.

\begin{property}[Forward and Backward arc]
  \label{prop:forward-backward-arc}
  An arc \(\left( i, j \right)\) in a \(s-t\) cut, where \(s \in S,\ t \in S^\prime\), is called:
  \begin{itemize}
    \item \textbf{forward} if \(s \in S, t \in S^\prime\)
    \item \textbf{backward} if \(s \in S^\prime, t \in S\)
  \end{itemize}
\end{property}

\subparagraph*{Idea}
start from a feasible flow \(x\) and try to iteratively increase its value \(\phi\) by sending, at each iteration, an additional amount of product along a directed \textit{(or undirected)} path from \(s\) to \(t\) with a strictly positive residual capacity.

\begin{itemize}
  \item If \(\left( i, j \right)\) is \textbf{not saturated} \textit{(i.e. \(x_{ij} < k_{ij}\))}, then \(\left( i, j \right)\) is called a \textbf{residual arc} and \(x_{ij}\) can be increased
  \item If \(\left( i, j \right)\) is \textbf{not empty} \textit{(i.e. \(x_{ij} \geq 0\))}, then \(\left( i, j \right)\) is called a \textbf{reverse arc} and \(x_{ij}\) can be decreased while respecting the flow balance equations
\end{itemize}

The following Property \textit{(\ref{prop:augmenting-path})} is the key to the Ford-Fulkerson algorithm.
\begin{property}[Augmenting Path]
  \label{prop:augmenting-path}
  A path \(P\) from \(s\) to \(t\) is an \textbf{augmenting path} with respect to the current feasible flow \(x\) if \(x_{ij} <k_{ij}\) for any forward arc and \(x_{ij} > 0\) for any backward arc.
\end{property}

The the algorithm works by sending \(\delta\) additional units of product can be sent from \(s\) to \(t\):

\begin{itemize}
  \item \(+\delta\) along \textbf{forward} arcs
  \item \(-\delta\) along \textbf{backward} arcs
\end{itemize}

\bigskip
Given a feasible flow \(x\) for \(G = (V, A)\), a \textbf{residual network} \(\overline{G} = \left(V, \overline{A}\right)\) associated to \(x\) is defined as follows:

\begin{itemize}
  \item If \(\left( i, j \right) \in A\) is not empty, \(\left( j, i \right) \in \overline{A}\) with \(\overline{k}_{ij} = x_{ij} > 0\)
  \item If \(\left( i, j \right) \in A\) is not saturated, \(\left( i, j \right) \in \overline{A}\) with \(\overline{k}_{ij} = k_{ij} - x_{ij} > 0\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(\overline{k}_{ij}\) is called the \textbf{residual capacity} of \(\left( i, j \right)\)
        \end{itemize}
\end{itemize}

At each iteration:

\begin{enumerate}[label=step \arabic*., ref=step \arabic*, leftmargin=*, labelindent=1em, widest*=8]
  \item \textbf{Find} an augmenting path \(P\) from \(s\) to \(t\)
  \item \textbf{Send} \(\delta\) units of product along \(P\)
  \item \textbf{Update} the feasible flow \(x\) and the residual network \(\overline{G}\)
\end{enumerate}

\subparagraph*{Goal}
\begin{itemize}[label=\(\rightarrow\)]
  \item \textit{Input}: graph \(G = (N. A)\) with capacity \(k_{ij} > 0\) for any \((i, j) \in A, \ s,t \in N\)
  \item \textit{Output}: a feasible flow \(x\) from \(s\) to \(t\) of maximum value \(\phi^\ast\)
\end{itemize}

The pseudocode is shown in Code~\ref{code:ford-fulkerson}.

\begin{lstlisting}[caption={Ford-Fulkerson algorithm}, label={code:ford-fulkerson}]
x := 0
phi := 0
optimum := false
do
  build residual network $\overline{\texttt{G}}$ associated to x
  P := path from s to t in $\overline{\texttt{G}}$
  if P is not defined then
    optimum := true
  else
    delta := min residual capacity of arcs in P
    phi := phi + delta
    for each arc (i, j) in P do
      if (i, j) is a forward arc then
        x_ij := x_ij + delta
      else
        x_ij := x_ij - delta
      end
    end
  end
until optimum = true
\end{lstlisting}

\subparagraph*{Complexity}
\begin{itemize}
  \item Since \(\delta > 0\), the value \(\phi\) increases at each iteration
  \item If \(k_{ij} \in \N \, \forall \, i, j\), \(x \in \N\overline{k}_{ij} \in \N\), and \(\delta \geq 1\), then there are at most \(\phi^\ast\) increases
  \item Since
        \[ \phi^\ast \leq k\left( \left\{ s \right\} \right) \leq m k_{max}, \quad m = |A|, \quad k_{max} = \max\left\{ k_{ij} \mid \left( i, j \right) \in A \right\} \]
        and each cycle is \(\bigO{m}\), the overall complexity is \(\bigO{m k_{max}^2}\).
\end{itemize}

\textbf{Space complexity}:
\begin{itemize}
  \item The size of an instance \(I\), written as \(|I|\), is the number of bits needed to represent \(I\)
  \item Since \(\left\lceil{\log_2{i}}\right\rceil + 1\) bits are needed to store an integer \(i\), \(|I| = \bigO{m \log_2{\left( k_{max} \right)}}\)
  \item \(\bigO{m^2 k_{max}}\) grows exponentially with \(|I|\) because \(k_{max} = 2^{\log_2{k_{max}}}\) ??? % TODO fix this, as the slides are wrong
\end{itemize}

\paragraph{Correcteness of the Ford-Fulkerson algorithm}

\begin{proposition}
  The Ford-Fulkerson algorithm is exact
\end{proposition}

\begin{proof}
  A feasible flow \(x\) has a maximum value if and only if \(t\) is not reachable from \(s\) in the residual network \(\overline{G}\) associated to \(x\).

  \begin{itemize}
    \item[\(\Rightarrow\)] If exists an augmenting path, then \(x\) is not optimal
    \item[\(\Leftarrow\)] If \(t\) is not reachable from \(s\) in \(\overline{G}\), then there is a cut of \(\overline{G}\) such that \(\delta^+_{\overline{G}}(S^\ast) = \emptyset\)
  \end{itemize}

  By definition of \(\overline{G}\), every arc \(\left( i, j \right) \in \delta^+_{\overline{G}}(S^\ast)\) is saturated, while every arc \(\left( i, j \right) \in \delta^-_{\overline{G}}(S^\ast)\) is empty.
  Therefore:
  \[ \phi(S^\ast) = \displaystyle \sum_{\left( i, j \right) \in \delta^+_G(S^\ast) } x_{ij} - \displaystyle \sum_{\left( i, j \right) \in \delta^-_G(S^\ast) } x_{ij} = \displaystyle \sum_{\left( i, j \right) \in \delta^+_G(S^\ast) } k_{ij} = k(S^\ast) \]

  By weak duality Property \textit{(\ref{prop:weak-duality})}, \(\phi(S^\ast) \leq k(S^\ast) \, \forall \, \text{ feasible}, \, \forall \, S \in V, s\in S, t \notin S\).
  Then, the flow \(x\) has maximum value and the cut induced by \(S^\ast\) has minimum capacity.
\end{proof}

\paragraph{Strong duality of the Ford-Fulkerson algorithm}

The \textit{Ford-Fulkerson} algorithm implies the following Theorem \textit{(\ref{thm:strong-duality})}.

\begin{theorem}[Strong duality]
  The value of a feasible flow of maximum value is equal to the capacity of a cut of minimum capacity.
  \label{thm:strong-duality}
\end{theorem}

\subparagraph*{Remarks}
\begin{itemize}
  \item If all the capacities \(k_{ij} \in \mathbb{Z}^+\), the flow \(x\) of maximum value has all \(x_{ij}\) integer and an integer value \(\phi^\ast\)
  \item \textit{Ford-Fulkerson} algorithm is not greedy \textit{(\(x_{ij}\) can be increased or decreased)}
\end{itemize}

\paragraph{Polynomial time algorithms for flow problems}

More efficient algorithms exists, based on augmenting paths, pre flows and capacity scaling.

\subparagraph*{Idea}
Start from a feasible flow \(x\) of value \(\phi\) and send, at each iteration, an additional amount of product in the residual network, respecting the residual capacities and the value \(\phi\), along cycles of negative cost.

\subsubsection{Indirect applicaton - assignment problem}

A common indirect application of the Ford-Fulkerson algorithm is the assignment (or matching) problem:

Given \(m\) engineers, \(n\) tasks and for each engineer the list of tasks they can perform.
Assign the task to each engineer such that:

\begin{itemize}
  \item each engineer is assigned at most one task
  \item each task is assigned to at most one engineer
  \item the tasks assigned to each engineer are the ones they can perform
\end{itemize}

\subparagraph*{Graphical model}
bipartite graph of competences.
An example is shown in Figure \ref{fig:assignment-example}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{figure-21.tikz}
  \caption{Example of assignment problem}
  \label{fig:assignment-example}
  \bigskip
\end{figure}

\bigskip
\begin{definition}[Matching]
  Given an undirected graph \(G = \left( V, E \right)\), a \textbf{matching} is a subset \(M \subseteq E\) such that:
  \begin{itemize}
    \item \(M\) is a set of edges
    \item each vertex of \(V\) is incident to at most one edge of \(M\)
  \end{itemize}
  \label{def:matching}
\end{definition}

Thanks to Definition~\ref{def:matching} the assignment problem can be solved by finding a maximum matching in the bipartite graph of competences;
this equates to finding a feasible flow of maximum value from \(s\) to \(t\) in the bipartite graph of competences.
An example of this problem is shown in Figure \ref{fig:assignment-flow-example}.

There is a correspondence between the feasible flows from source to sink of value \(\phi\) and the matchings contained the edges of \(\phi\).

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{figure-22.tikz}
  \caption{Example of assignment problem solved by flow}
  \label{fig:assignment-flow-example}
  \bigskip
\end{figure}

\clearpage

\section{Linear Programming}
\label{sec:linear-programming}

\subsection{Optimization problems}

\textbf{Optimization problems} are problems that require to find the best solution among a set of possible solutions.
A formal definition is shown in \ref{def:instance-lp} and \ref{def:linear-programming-problem}.

\bigskip
\begin{definition}[Instance of a Linear Programming problem]
  \label{def:instance-lp}
  The \textbf{instance} of an optimization problem is a pair \(\left( F, c \right)\) where:
  \begin{itemize}
    \item \(F\) is the \textbf{domain} of feasible point
    \item \(c\) is the \textbf{cost function}, a mapping \(c :  F \rightarrow \R\)
    \item the problem is finding an \(f \in F\) such that \(c\left( f \right) \leq c\left( y \right) \, \forall \, y \in F\)
          \begin{itemize}
            \item such point is called a \textbf{globally optimal} \textit{(or just optimal)} solution to the given instance.
          \end{itemize}
  \end{itemize}

  An \textbf{optimization problem} is a set of \(I\) instances of a given optimization problem.
\end{definition}

\begin{definition}[Linear Programming Problem]
  \label{def:linear-programming-problem}
  A \textbf{linear programming} \textit{(or \LP)} \textbf{problem} is an optimization problem such as

  \begin{gather*}
    \begin{aligned}
      \min \         & f(x)                                  \\
      \text{s.t.} \  & x \in X \subseteq \R^n \rightarrow \R
    \end{aligned}
    \label{eq:linear-programming}
  \end{gather*}

  where:

  \begin{itemize}
    \item the \textbf{objective function} \(f: X \rightarrow \R\) is \textbf{linear}
    \item the \textbf{feasible region} \(X \left\{ x \in \R^n \mid   g_i(x) \, r_i \, 0 \land i \in \left\{ 1, \, \ldots \,, m \right\}\right\}\) with: \[r_i \in \left\{ =, \geq, \leq \right\} \ \text{and} \ g_i: \R^n \rightarrow \R\] are \textbf{linear} functions for \(i \in \left\{ 1, \, \ldots \,, m \right\}\)

    \item \(x^\ast \in \R^n\) is an \textbf{optimal solution} of the \LP \ref{eq:linear-programming} if \(f(x^\ast) \leq f(x) \, \forall \, x \in X\)
  \end{itemize}
\end{definition}

\bigskip
A wide variety of decision making problems can be formulated or approximated as \LP, as they often involve the optimal allocation of a given set of limited resources to different activities.

\bigskip
Two forms of \LP are commonly used:

\begin{itemize}
  \item \textbf{General form} of a linear programming problem:
        \begin{gather*}
          \begin{aligned}
            \min \         & z = c_1 x^1 + \cdot + c_n x_n                                    \\
            \text{s.t.} \  & a_{11} x^1 + \cdot + a_{1n} x_n \left( \leq, =, \geq \right) b_1 \\
                           & \vdots                                                           \\
                           & a_{m1} x^1 + \cdot + a_{mn} x_n \left( \leq, =, \geq \right) b_m \\
                           & x^1, \, \ldots \,, x_n \geq 0
          \end{aligned}
        \end{gather*}

  \item \textbf{Matrix notation} of a linear programming problem:
        \begin{gather*}
          \begin{aligned}
            \min \          z = & \begin{bmatrix}
                                    c_1 & c_2 & \cdots & c_n
                                  \end{bmatrix}
            \begin{bmatrix}
              x_1 \\ x_2 \\ \vdots \\ x_n
            \end{bmatrix}                                                        \\
            \text{s.t.} \       & \begin{bmatrix}
                                    a_{11} & a_{12} & \cdots & a_{1n} \\
                                    a_{21} & a_{22} & \cdots & a_{2n} \\
                                    \vdots & \vdots & \ddots & \vdots \\
                                    a_{m1} & a_{m2} & \cdots & a_{mn}
                                  \end{bmatrix}
            \begin{bmatrix}
              x_1 \\ x_2 \\ \vdots \\ x_n
            \end{bmatrix} \left\{ \leq, =, \geq \right\}
            \begin{bmatrix}
              b_1 \\ b_2 \\ \vdots \\ b_m \end{bmatrix}                                          \\
                                & \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}
            \geq 0
          \end{aligned}
        \end{gather*}
\end{itemize}

\subsection{Assumptions of \LP models}

The \LP model is based on the following \textbf{assumptions}:

\begin{itemize}
  \item \textbf{Linearity} \textit{(proportionality and additivity)} of the objective function and constraints
        \begin{itemize}[label=\(\rightarrow\)]
          \item \textbf{proportionality}: \( \textit{contribution of each variable} \, = \, \textit{constant} \, \times \ \textit{variable} \)
                \begin{itemize}[label=\xmarkthin]
                  \item It does not account for economies of scale
                \end{itemize}
          \item \textbf{additivity}: \(\textit{total contribution} \, =\sum_i \textit{contribution of each variable} \ i\)
                \begin{itemize}[label=\xmarkthin]
                  \item It does not account for competing activities \textit{(their sum is not necessarily the total contribution)}
                \end{itemize}
        \end{itemize}
  \item \textbf{Divisibility} of the variables, as they can assume fractional \textit{(rational)} values
  \item \textbf{Parameters} are assumed to be \textbf{constants} that can be estimated with a sufficient degree of accuracy
        \begin{itemize}[label=\xmarkthin]
          \item more complex mathematical programs are needed to account for uncertainty in the parameter values
        \end{itemize}
\end{itemize}

The \textbf{sensitivity analysis} is a technique that allows to evaluate how \inlinequote{sensitive} an optimal solution is with respect to small changes in the parameters of the model;
it will be introduced in Section~\ref{sec:sensitivity-analysis}.

\subsection{Equivalent Forms}

The \textbf{General Form} \textit{(or canonical form)} \textit{(Definition~\ref{def:linear-programming-general-form}) of a \LP can be expressed in the equivalent \textbf{Standard Form} (Definition~\ref{def:linear-programming-standard-form})}.

\begin{definition}[General form of a \LP problem]
  \label{def:linear-programming-general-form}
  The \textbf{General Form} of a \LP is:
  \begin{align*}
    \min\ (\max) \quad & z = c^T x                                                                                                      \\
    \text{s.t.} \quad  & A_1 x \geq b_1 \quad                                                       & \text{inequality constraints}     \\
                       & A_2 x \leq b_2 \quad                                                       & \text{inequality constraints}     \\
                       & A_3 x = b_3 \quad                                                          & \text{equality constraints}       \\
                       & x_j \geq 0,\ j \in J \subseteq \left\{ 1, \, \ldots \,, n \right\} \quad   & \text{non-negativity constraints} \\
                       & x_j \ \text{free}, \ j \in \left\{ 1, \, \ldots \,, n \right\} \setminus J & \text{free variables}
  \end{align*}
\end{definition}

\begin{definition}[Standard Form of a \LP problem]
  \label{def:linear-programming-standard-form}
  The \textbf{Standard Form} of a \LP is the following:
  \begin{align*}
    \min\ (\max) \quad & z = c^T x &                               \\
    \text{s.t.} \quad  & Ax = b    & \text{equality constraints}   \\
                       & x \geq 0  & \text{non negative variables}
  \end{align*}
\end{definition}

The two forms are \textbf{equivalent}, as simple transformation rules allow to pass from one form to the other;
the transformations may involve adding and or deleting variables and constraints, as the next Section shows.

\subsubsection{Transformation rules}

\begin{itemize}[parsep=1.25ex]
  \item \(\max c^T x \Rightarrow \min -c^T x\)
  \item \(a^T x \leq b \Rightarrow \begin{cases}
          a^T x + s = b \\
          s \geq 0
        \end{cases}\) s is a \textbf{slack variable}
  \item \(a^T x \geq b \Rightarrow \begin{cases}
          a^T x - s = b \\
          s \geq 0
        \end{cases}\) s is a \textbf{surplus variable}
  \item \(x_j\) \textbf{unrestricted} in sign \(\Rightarrow \begin{cases}
          x_j = x_j^+ - x_j^- \\
          x_j^+ \geq 0        \\
          x_j^- \geq 0
        \end{cases}\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item after the substitution, \(x_j\) is deleted from the problem
        \end{itemize}
  \item \(a^T x \leq b \Leftrightarrow - a^T x \geq -b\)
  \item \(a^T x \geq b \Leftrightarrow - a^T x \leq -b\)
  \item \(a^T x = b \Leftrightarrow \begin{cases}
          a^T x \leq b \\
          a^T x \geq b
        \end{cases} \Leftrightarrow \begin{cases}
          a^T x \leq b \geq b \\
          -  a^T x \geq - b
        \end{cases}\)
\end{itemize}

\subsection{Graphical solutions}

\begin{definition}[Level curve]
  \label{def:level-curve}
  A \textbf{level curve} of value \(z\) of a function \(f\) is the set of points \(P\) in \(\R^n\) where \(f\) is constant and has value \(z\):
  \[P = \left\{ x \in \R^n \mid f(x) = z \right\}\]
\end{definition}

\bigskip
Consider a \LP with inequality constraints \textit{(as it's easier to visualize)}.

\begin{itemize}
  \item A \textbf{hyperplane} is the set of points that satisfy the constraint
        \( H = \left\{ x \in \R^n \mid   a^T x = b \right\} \)
  \item An \textbf{affine half space} is the set of points that satisfies the constraint
        \(H = \left\{ x \in \R^n \mid   a^T x \leq b \right\} \)
        \begin{itemize}[label=\(\rightarrow\)]
          \item each inequality constraint \(a^T x \leq b\) defines an affine half space in the variable space
          \item in \(R^2\), an affine half space is a is a \textbf{half plane}
        \end{itemize}
  \item The \textbf{feasible region} \(X\) of any \LP is a polyhedron \(P\) defined by the intersection of a finite number of affine half spaces
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(P\) can be \textbf{empty} or \textbf{unbounded}
        \end{itemize}
  \item A subset \(S \subseteq \R^n\) is \textbf{convex} if for each pair of points \(y^1, y^2 \in S\) the line segment between \(y^1\) and \(y^2\) is contained in \(S\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item given two points \(y^1, y^2 \in S\), a \textbf{convex combination} of them is any point of the form
                \[ z = \lambda y^1 + \left( 1 - \lambda \right) y^2 \quad 0 \leq \lambda \leq 1 \]
          \item a convex combination with \(\lambda \neq 0, 1\) is called \textbf{strict}
          \item a function \(f: \R^n \rightarrow \R^m\) is \textbf{convex} if the space above the graph of \(f\) is convex
        \end{itemize}
  \item The segment defined by all the convex combinations of \(y^1\) and \(y^2\), \(y^1, y^2 \in S\), is called a \textbf{convex hull}
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(\left[ y^1, y^2 \right] = \left\{ x \in \R^n \,\middle\vert\,   x = \alpha y^1 + \left( 1-\alpha \right) y^2 \land \alpha \in \left[ 0, 1 \right] \right\}\)
        \end{itemize}
  \item A \textbf{polyhedron} \(P\) is a convex set of \(\R^n\)
        \begin{itemize}
          \item any \textbf{half space} is \textbf{convex}
          \item the \textbf{intersection} of a finite number of convex sets is also a \textbf{convex} set
        \end{itemize}
  \item A \textbf{vertex of polyhedron} \(P\) is a point of \(P\) that cannot be expressed as a convex combination of other points of \(P\)
        \begin{itemize}
          \item \(x\) is a vertex \(P\) iff
                \[ x = \alpha y^1 + (1 - \alpha) y^2 \quad \alpha \in \left[ 0, 1 \right] \quad y^1, y^2 \in P \Rightarrow x = y^1 \lor x = y^2 \]
          \item a non empty polyhedron \(P = \left\{ x \in \R^n \mid  Ax = b, x \geq 0 \right\}\) has a finite number (\(n >1\)) of vertices
        \end{itemize}
  \item A \textbf{polytope} is a bounded polyhedron
  \item Let \(x\) be an element of a polyhedron \(P\). A vector \(d \in \R^n\) is said to be a \textbf{feasible direction} at \(x\) if there exists \(\theta \in \R, \theta > 0\) such that \(x + \theta d \in P\).
  \item Given a polyhedron \(P\), a vector \(d \in \R^n, d \neq \overline{0}\) is an \textbf{unbounded feasible direction} of \(P\) if, for every point \(x^0 \in P\), the ray \(\left\{ x \in \R^n \mid   x = x^0 + \lambda d, \lambda \geq 0 \right\}\) is contained in \(P\)
\end{itemize}

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \bigskip
    \tikzfig{figure-3.tikz}
    \caption{Hyperplane}
    \label{subfig:hyperplane}
    \bigskip
  \end{subfigure}
  \bigskip
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \bigskip
    \tikzfig{figure-2.tikz}
    \caption{Affine half space}
    \label{subfig:affine-half-space}
    \bigskip
  \end{subfigure}
  \begin{subfigure}[h]{0.495\textwidth}
    \centering
    \bigskip
    \tikzfig[0.75]{figure-4.tikz}
    \caption{Convex polyhedron}
    \label{subfig:convex-hull}
    \bigskip
  \end{subfigure}
  \begin{subfigure}[h]{0.495\textwidth}
    \centering
    \bigskip
    \tikzfig[0.75]{figure-5.tikz}
    \caption{Concave (not convex) polyhedron}
    \label{subfig:not-convex-hull}
    \bigskip
  \end{subfigure}
  \caption{Illustrations of \LP geometry definitions}
  \label{fig:lp-geometry}
  \bigskip
\end{figure}

\subsubsection{Weyl-Minkoswki Theorem}

The \textbf{Weyl-Minkoswki Theorem} describes the representation of a polyhedron.
Its formulation is expressed in Theorem~\ref{thm:weyl-minkowski}.

\begin{theorem}[Weyl-Minkoswki]
  Every point \(x\) of a polyhedron \(P\) can be expressed as a convex combination of its vertices \(x^1, \, \ldots \,, x^k\) plus \textit{(if needed)} an unbounded \textbf{feasible direction} \(d\) of \(P\):
  \[ x = \alpha_1 x^1 + \, \ldots \, + \alpha_k x^k + d \]
  where the multipliers \(\alpha_i \geq 0\) satisfy the constraint \(\displaystyle\sum_{i=1} \alpha_i = 1\).
  \label{thm:weyl-minkowski}
\end{theorem}

\bigskip
The unbounded feasible direction is needed if the polyhedron is unbounded; Figure~\ref{fig:weyl-minkowski-theorem} represent the cases of a bounded and an unbounded polyhedron.

\begin{figure}[htbp]
  \centering
  \bigskip
  \begin{subfigure}[h]{0.495\textwidth}
    \centering
    \bigskip
    \tikzfig{figure-6.tikz}
    \caption{Unbounded polyhedron}
    \label{subfig:unbounded-polyhedron}
    \bigskip
  \end{subfigure}
  \begin{subfigure}[h]{0.495\textwidth}
    \centering
    \bigskip
    \tikzfig{figure-7.tikz}
    \caption{Bounded polyhedron}
    \label{subfig:bounded-polyhedron}
    \bigskip
  \end{subfigure}
  \caption{Illustration of the \textit{Weyl-Minkowski} theorem}
  \label{fig:weyl-minkowski-theorem}
  \bigskip
\end{figure}

\subparagraph*{Consequences of the theorem}
Every point \(x\) of polytope \(P\) can be expressed as a convex combination of its vertices;
the \textit{Weyl-Minkowski theorem} can be used to describe any point.
An example of this is shown in Figure~\ref{fig:polytope-example}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.7]{figure-8.tikz}
  \caption{Example of polytope: \(x = \alpha_1 x^1 + \alpha_2 x^2 + \alpha_3 x^3\) with \(\alpha_1 + \alpha_2 + \alpha_3 = 1, \alpha_i \geq 0, d = 0\)}
  \label{fig:polytope-example}
  \bigskip
\end{figure}

\subsubsection{Geometry of \LP}

Geometrically:

\begin{itemize}
  \item An \textbf{interior point} \(x \in P\) cannot be an optimal solution of the problem
        \begin{itemize}
          \item it always exists an improving direction
          \item consider Figure~\ref{subfig:interior-point-solution} where \(c\) represents the direction of fastest increase in \(z\) \textit{(constant gradient)}
        \end{itemize}
  \item In an optimal vertex, all \textbf{feasible direction are worsening directions}
        \begin{itemize}
          \item consider Figure~\ref{subfig:worsening-point-direction} where \(c\) represents the direction of fastest increase in \(z\) \textit{(constant gradient)}
        \end{itemize}
  \item The \textit{Weyl-Minkowski} theorem implies that, although the variables can assume fractional values, any \LP can be seen as a \textbf{combinatorial problem}
        \begin{itemize}
          \item \inlinequote{only} the vertices of the polyhedron have to be considered in order to find the feasible solutions
          \item the graphical method in only applicable for \(n \leq 3\)
          \item the number of vertices often grows exponentially with respect to the number of variables
        \end{itemize}
\end{itemize}

\bigskip
Furthermore, let \(P\) be a convex polytope of dimension \(d\) and let \(HS\) be a half space of \(P\) defined by hyperplane \(H\).
If the intersection \(f = P \cap HS\) is a subset of \(H\) then:

\begin{itemize}
  \item \(f\) is a \textbf{face} of \(P\)
  \item \(H\) is the \textbf{support hyperplane} of \(f\)
\end{itemize}

The face is then defined according to its dimension:

\begin{itemize}
  \item a \textbf{vertex} is a face of dimension \(0\) \textit{(a point)}
  \item a \textbf{facet} is a face of dimension \(d-1\) \textit{(a hyperplane)}
  \item an \textbf{edge} is a face of dimension \(1\) \textit{(a line)}
\end{itemize}

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{subfigure}[b]{0.495\textwidth}
    \bigskip
    \centering
    \tikzfig{figure-9.tikz}
    \caption{Interior point of polyhedron \(P\)}
    \label{subfig:interior-point-solution}
    \bigskip
  \end{subfigure}
  \begin{subfigure}[b]{0.495\textwidth}
    \bigskip
    \centering
    \tikzfig{figure-10.tikz}
    \caption{Worsening directions of polyhedron \(P\)}
    \label{subfig:worsening-point-direction}
    \bigskip
  \end{subfigure}
  \bigskip
\end{figure}

\subsubsection{Four types of \LP}

There are four types of \LP, depending on the number of solutions; all are illustrated in Figure~\ref{fig:lp-types}.
Since the objective of the problem is to minimize \(f(x)\) \textit{(as it's in form \(\min c^T x\))}, better solutions are found by moving along the direction \(-c\) \textit{(the opposite of the gradient \(\nabla f(x)\))}.

\begin{enumerate}
  \item \textbf{A unique} optimal solution, Figure~\ref{subfig:unique-solution}
  \item \textbf{Multiple} \textit{(infinitely many)} optimal solutions, Figure~\ref{subfig:multiple-solutions}
  \item \textbf{Unbounded} \LP, Figure~\ref{subfig:unbounded-lp}
        \begin{itemize}
          \item this type of problem has unbounded polyhedron and unlimited objective function values
        \end{itemize}
  \item \textbf{Infeasible} \LP, Figure~\ref{subfig:infeasible-lp}
        \begin{itemize}
          \item this type of problem has an empty polyhedron and no feasible solution
        \end{itemize}
\end{enumerate}

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{subfigure}[b]{0.495\textwidth}
    \bigskip
    \centering
    \tikzfig{figure-11.tikz}
    \caption{A unique optimal solution}
    \label{subfig:unique-solution}
    \bigskip
  \end{subfigure}
  \begin{subfigure}[b]{0.495\textwidth}
    \bigskip
    \centering
    \tikzfig{figure-12.tikz}
    \caption{Multiple optimal solutions}
    \label{subfig:multiple-solutions}
    \bigskip
  \end{subfigure}
  \bigskip
  \begin{subfigure}[b]{0.495\textwidth}
    \bigskip
    \centering
    \tikzfig{figure-13.tikz}
    \caption{Unbounded \LP}
    \label{subfig:unbounded-lp}
    \bigskip
  \end{subfigure}
  \begin{subfigure}[b]{0.495\textwidth}
    \bigskip
    \centering
    \tikzfig{figure-14.tikz}
    \caption{Infeasible \LP}
    \label{subfig:infeasible-lp}
    \bigskip
  \end{subfigure}
  \caption{Four types of \LP}
  \label{fig:lp-types}
  \bigskip
\end{figure}

\subsubsection{Basic feasible solutions and polihedra vertices}

Due to the fundamental theorem of Linear Programming \textit{(Theorem~\ref{thm:weyl-minkowski})}, to solve any \LP problem it suffices to consider the \textit{(finitely many)} vertices of the polyhedron \(P\) of feasible solutions;
since the geometrical definition of vertex cannot be exploited algorithmically, an algebraic definition is needed.

A vertex corresponds to the intersection of the hyperplanes associated to \(n\) inequalities.
If a polyhedron is expressed in standard form:
\[ P = \left\{ x \in \R^n \mid  Ax = b, x \geq 0 \right\}\]
At is possible to transform it into a inequality \textit{(as it is easier to work with)}:
\[ P = \left\{ x \in \R^n \mid  Ax \leq b, x \geq 0 \right\}\]
And later transform it back into standard form, adding a slack variable \(s\):
\[ P^T = \left\{ x \in \R^n \mid   Ax -s = b, s \geq 0, x \geq 0 \right\} \]
where \(P^T\) is the polyhedron of feasible solutions of the original problem.
Finally, by defining the matrices:
\[ A \coloneqq \left[ \, A \,|\, I \, \right], \quad x \coloneqq \left( \, x^T \,|\, s^T \right) \]
the system of equation is represented in matrix form, where \(A\) has \(m\) rows and \(n\) columns.

\bigskip
\begin{property}
  For any polyhedron \(P = \left\{ x \in \R^n \mid   Ax = b, x \geq 0 \right\}\), where \(A\) has \(m\) rows:

  \begin{itemize}
    \item the \textbf{facets} (edges in \(\R^2\)) are obtained by setting one variable to \(0\)
    \item the \textbf{vertices} are obtained by settings \(n-m\) variables to \(0\)
  \end{itemize}
\end{property}

\paragraph{Optimality of extreme points}

\begin{theorem}
  Consider the \LP problem of minimizing \(c^T x\) over a polyhedron \(P\).
  Suppose that \(P\) has at least one extreme point and that there exists an optimal solution.
  Then, there exists and optimal solution which is an extreme point of \(P\).
  \label{thm:extreme-points-solution}
\end{theorem}

The above Theorem~\ref{thm:extreme-points-solution} \textbf{applies to polyhedra in standard form}, as well as to bounded polyhedra \textit{(as they don't contain a line)}.

\begin{theorem}
  Consider the \LP problem of minimizing \(c^T x\) over a polyhedron \(P\).
  Suppose that \(P\) has at least one extreme point.
  Then, either the optimal cost is equal to \(-\infty\), or there exists an extreme point which is optimal.
  \label{thm:extreme-points-solution-2}
\end{theorem}

For a general \LP problem. if the feasible set has no extreme point, then Theorem~\ref{thm:extreme-points-solution-2} does not apply;
however, the problem can be transformed into an equivalent one in standard form, establishing the following Corollary.

\begin{corollary}
  Consider the \LP problem of minimizing \(c^T x\) over a polyhedron \(P\).
  Suppose that \(P\) has at least one extreme point.
  Then, either the optimal cost is equal to \(-\infty\), or there exists an optimal solution.
  \label{cor:extreme-point-solution-2}
\end{corollary}

\subsubsection{Algebraic characterization of vertices}

Consider any polyhedron \(P = \left\{ x \in \R^n \mid   Ax = b, x \geq 0 \right\}\), in standard form.

\subparagraph*{Assumption}
\(A \in \R^{m \times n}\) is such that \(m \leq n\) of rank \(m\) \textit{(i.e. \(A\) is full rank)}.
This is equivalent to assume that there are no redundant constraints.

\subparagraph*{Solutions}
\begin{itemize}
  \item If \(m = n\), there is a unique solution of \(Ax = b\) \textit{(\(x = A^{-1} b)\)}
  \item If \(m < n\), there are \(\infty^{n-m}\) solutions of \(Ax = b\)
        \begin{itemize}
          \item the system has \(n-m\) \textbf{degrees of freedom}
          \item by fixing the degrees of freedom to \(0\), a \textbf{vertex} is obtained
        \end{itemize}

        The \textbf{basis} of matrix \(A\) is a subset of \(m\) columns of \(A\) that are linearly independent and form an \(m \times m\) non singular matrix \(B\).
        \[ A = [\, B \mid N \, ] \]
        where:
        \begin{align*}
          B \quad & \text{has} \ m \ \text{columns}   \\
          N \quad & \text{has} \ n-m \ \text{columns}
        \end{align*}
\end{itemize}

\subsubsection{Basic solutions}
\label{sec:basic-solutions}

Consider the vector \(x^T = \left[ \ x_B^T \ \middle\vert \ x_N^T \ \right]\) where:

\begin{align*}
  x_B^T \quad & \text{has} \ m \ \text{components}   \\
  x_N^T \quad & \text{has} \ m-n \ \text{components}
\end{align*}

Then any system \(Ax = b\) can be written as \(B _{x_B} + N_{x_N} = b\), and for any set of values of \(x_N\), if \(B\) is not singular (if \(\det(B) \neq 0\)), then \(x_B = B^{-1} (b - N_{x_N})\).

\subparagraph*{Definitions}
\begin{itemize}
  \item A \textbf{basic solution} is a solution obtained by setting \(x_N = 0\) and, consequently, \(x_B = B^{-1} b\)
        \begin{itemize}
          \item two basic solutions are called adjacent if there are \(n-1\) linearly independent constraints active in both of them
        \end{itemize}
  \item A \textbf{feasible solution} is any vector \(x \geq 0\) such that \(Ax = b\)
  \item An \textbf{unfeasible solution} is any vector \(x < 0\) such that \(Ax = b\)
  \item A feasible solution with \(x_B \geq 0\) is a \textbf{basic feasible solution} with basis \(B\)
        \begin{itemize}
          \item \(\forall \, j \notin B, \ x_j = 0\) \textit{(all the non zero variables are in \(B\))}
          \item a \textbf{basic feasible solution} has \(n\) linearly independent constants
          \item the variables in \(x_B\) are the \textbf{basic variables} and those in \(x_N\) are non \textbf{basic variables}
          \item by construction, \(\left( x_B^T \, , \, x_N^T \right)\) satisfy \(Ax = b\)
        \end{itemize}
  \item A basic feasible solution is \textbf{degenerate} if it contains at least one basic variable with value \(0\)
        \begin{itemize}
          \item a basis uniquely determines a basic solution
          \item two different bases may lead to the same basic solution
        \end{itemize}
\end{itemize}

\begin{theorem}[Basic feasible solutions]
  \(x \in \R^n\) is a \textbf{basic feasible solution}, or \BFS, if and only if \(x\) is a vertex of the polyhedron \(P = \left\{ x \in \R^n \mid   Ax = b, x \geq 0 \right\}\)
\end{theorem}

\begin{property}[Number of solutions]
  At most, there exists one basic feasible solution for each choice of the \(n-m\) non basic variables out of the \(n\) variables:
  \[\text{\# basic feasible solutions} \leq
    \begin{pmatrix}
      n \\ n-m
    \end{pmatrix}
    = \dfrac{n!}{\left( n - m \right)! \left( n - \left( n-m \right) \right)!} =
    \begin{pmatrix}
      n \\ m
    \end{pmatrix}
  \]
\end{property}

\subsection{Simplex method}

The Simplex method provides a systematic way to find the optimal solution of a \LP problem;
given an \LP in standard form:

\begin{align*}
  \min \quad        & z = c^T x \\
  \text{s.t.} \quad & Ax = b    \\
                    & x \geq 0
\end{align*}

it examines a sequence of \textbf{basic feasible solutions} with non increasing objective function values, until an optimal solution is reached or the problem is found to be unbounded.
At each iteration, the basic feasible solution is replaced by a new one that is closer to the optimal solution.

In other, \textit{simpler}, words, it generates a path \textit{(a sequence of adjacent vertices)} along the edges of the polyhedron \(P\) that leads to the optimal solution.

\bigskip
The simplex method is articulated in \(3\) steps:

\begin{enumerate}[label=\arabic*., ref=(\arabic*)]
  \item Find an \textbf{initial vertex} or establish that the \LP is \textbf{unbounded}
  \item Determine whether the current vertex is \textbf{optimal}
  \item \label{enum:simplex-method-3} Move from a current vertex to a \textbf{better adjacent vertex} \textit{(in terms of objective function value)} or establish that the \LP is unbounded
\end{enumerate}

\bigskip
The solution found in step~\ref{enum:simplex-method-3} may be optimal only in a local sense;
in \LP, however, local optimality implies global optimality thanks to the following Property.

\begin{property}[Optimality of a vertex]
  If the \LP problem requires minimizing \textit{(or maximizing)} a convex function over a convex set, then the local optimum is also the global optimum.
  \label{prop:optimal-vertex}
\end{property}

\subsubsection{Optimality condition of a \LP solution}

Given a \LP:

\begin{align*}
  \min \quad        & z = c^T x \\
  \text{s.t.} \quad & Ax = b    \\
                    & x \geq 0
\end{align*}

Suppose that \(x \in P\) is a basic feasible solution and move away from it in the direction of a vector \(d \in \R^n\);
clearly, \(d\) should not immediately lead to a point outside \(P\), otherwise the solution would not be feasible any more.

\begin{definition}[Feasible direction]
  \label{def:feasible-direction}
  Let \(x\) be an element of polyhedron \(P\).
  A vector \(d \in \R^n\) is said to be a \textbf{feasible direction} at \(x\) if there exists a positive scalar \(\theta\) such that \(x + \theta d \in P\).
\end{definition}

\bigskip
Let \(x\) be a basic feasible solution to the standard form problem, let \(B(1), \, \ldots \,, B(m)\) the indices of the basic variables, and let \(B = \left[ A_{B(1)}, \, \ldots \,, A_{B(n)} \right]\) be the corresponding basis matrix.

In particular, \(x_i = 0\) for all non basic variables, while the vector \(x_B = \left( x_{B(1)}, \, \ldots \,, x_{B(m)} \right)\) of basic variables if given by
\[x_B = B^{-1}b\]

By selecting a non basic variable \(x_j\) (initially at zero) and increasing its value to \(\theta\) while keeping all the other non basic variables to zero, a new basic feasible solution \(x + \theta d\) is obtained.
Algebraically:
\[ d_i = \begin{cases}
    1 \quad & \text{ if } i = j  \\
    0 \quad & \text{ otherwise }
  \end{cases} \]
At the same time, the vector \(x_B\) changes to \(x_B + \theta d_B\), where \(d_B = (d_{B(1)}, \, \ldots \,, d_{B(m)})\) is the vector with those components of \(d\) that correspond to the basic variables.

Given that only the feasible solutions are relevant, it's necessary true that \(A \left( x + \theta d \right) = b\) and since \(x\) is feasible, \(A x = b\).
At the same time, since \(\theta > 0\), it must hold
\begin{equation}
  \label{eq:feasible-direction-1}
  A d = 0
  \tag{A}
\end{equation}

Since \(d_j = 1\) and \(d_i = 0 \, \forall \, i \neq j\):
\[ 0 = Ad = \displaystyle \sum_{i=1}^n A_i d_i = \sum_{i=1}^{m} A_{B(i)} d_{B(i)} = B d_B + A_j \]
due to the invertibility of the basis matrix \(B\):

\begin{equation}
  \label{eq:feasible-direction-2}
  d_B = - B^{-1} A_j
  \tag{B}
\end{equation}

The direction vector \(d\) will be referred thereon to as the \(j\)-th basic direction.

While the equality constraints are respected, the non negativity constraints are not.
During this movement, all the non basic variables stay at zero, while \(x_j\) is incremented: the focus is then on the basic variables.
Two cases are possible:

\begin{enumerate}[label=\arabic*., ref=(\arabic*)]
  \item \(x\) is a non degenerate basic solution: \(x_B > 0\), so \(x_B + \theta d_B \geq 0\) and feasibility is maintained with \(\theta\) sufficiently small. \(d\) is a \textbf{feasible direction} at \(x\).
  \item \(x\) is degenerate: it's possible that a basic variable \(x_{B(i)}\) is zero, while the correspondent component \(d_{B(i)}\) of \(d_{B} = -B^{-1}A_j < 0\). In this case, following the \(j\)-th basic direction, the non negativity constraint is violated and the solution is not feasible.
\end{enumerate}

\bigskip
Now let \(d\) be the \(j\)-th basic direction at \(x\).
The rate of change of the objective function along \(d\) is given by \(c^T_B d_B + c_j\), where \(c_B = \left( c_{B(1)}, \, \ldots \,, c_{B(,)} \right)\).

Using Equation~\ref{eq:feasible-direction-2}, the rate of change of the objective function along \(d\) is:
\[ c_j - c^T_B B^{-1} A_j \]
While quantity is important enough to warrant a Definition \textit{(\ref{def:reduced-cost})},
intuitively, \(c_j\) is the cost per unit increase in the variable \(x_j\), and the term \(-c^T_B B^{-1} A_j\) is the cost of the compensating change in the basic variables necessitated by the constraint \(Ax = b\).

\begin{definition}[Reduced costs]
  Let \(x\) be a basic solution with an associated basis matrix \(B\).
  Let \(c_B\) be the vector of costs of the basic variables.
  For each \(j\), the \textbf{reduced costs} \(\overline{c}_j\) of the non basic variable \(x_j\) are defined as
  \[ \overline{c}_j = c_j - c^T_B B^{-1} A_j \]
  \label{def:reduced-cost}
\end{definition}

\bigskip
Consider Definition~\ref{def:reduced-cost} for the case of a basic variable.
Since \(B\) is the matrix \(\left[ A_{B(1)}, \, \ldots \,, A_{B(m)} \right]\), then \(B^{-1}[A_{B(1)}, \, \ldots \,, A_{B(m)}] = I\) \textit{(is the identity matrix)}.
In particular, \(B^{-1} A_{B(i)}\) is the \(i\)-th column of the identity matrix, which is the \(i\)-th unit vector\(e_i\).

Therefore, for every basic variable \(x_{B(i)}\), the reduced cost is:
\[ \overline{c}_{B(i)} = c_{B(i)} - c^T_B B^{-1} A_{B(i)} = c_{B(i)} - c^T_B e_i = c_{B(i)} -  c_{B(i)} = 0 \]
so the reduced cost of a basic variable is always zero.
The next Theorem \textit{(\ref{thm:reduced-cost})} shows that the reduced cost of a non basic variable is always non negative.

\begin{theorem}
  Let \(x\) be a basic feasible solution \(x\) associated with a basis matrix \(B\), and let \(\overline{c}\) be the corresponding vector of reduced costs.
  Then:

  \begin{enumerate}
    \item If \(\overline{c} \geq 0\), then \(x\) is optimal
    \item If \(x\) is optimal and non degenerate, then \(\overline{c} = 0\)
  \end{enumerate}
  \label{thm:reduced-cost}
\end{theorem}

\bigskip
Note that Theorem~\ref{thm:reduced-cost} allows the possibility that \(x\) is a degenerate solution and \(\overline{c} < 0\) for some non basic index \(j\).
Furthermore, it states that in order to decide whether a non degenerate basic feasible solution is optimal, it's sufficient to check whether all the reduced costs are non negative, which is the same as examining \(n-m\) basic directions;
checking if the degenerate basic feasible solution \(x\) is optimal is not as simple.

Finally Definition~\ref{def:optimal-matrix} states the requirements for a matrix to be optimal.

\begin{definition}[Optimal matrix]
  A matrix \(B\) is \textbf{optimal} if and only if it is a basis matrix for an optimal basic feasible solution, i.e. the following conditions are both satisfied:

  \begin{itemize}
    \item \(B^{-1} b \geq 0\)
    \item \(\overline{c}^T = c^T - c^T_B B^{-1} A \geq 0\)
  \end{itemize}
  \label{def:optimal-matrix}
\end{definition}

\subparagraph*{Remarks}
\begin{itemize}[label=\(\rightarrow\)]
  \item For \textbf{maximization} problems, the condition is \(\overline{c}_N \leq 0\)
  \item This optimality condition is \textbf{sufficient} but generally not necessary
\end{itemize}

\subsubsection{General case}

Let's first of all assume that every basic feasible solution is non degenerate;
this assumption will be relaxed later on.

Suppose that the current basic feasible solution is \(x\), with reduced costs \(\overline{c}_j\) for each of the non basic variables.
If all of them are non negative, then \(x\) is optimal \textit{(thanks to Theorem~\ref{thm:reduced-cost})} and the algorithm can terminate.

Otherwise, if \(\exists \, \overline{c}_j < 0\) for a non basic variable \(x_j\), then the \(j\)-th basic direction \(d\) is a feasible direction of cost decrease;
while moving along it, \(x_j\) becomes positive and the value of all other non basic variables stays constant at \(0\).
This situation is described as \inlinequote{entering the basis}.

By moving away from \(x\) along \(d\), the traced points are in form \(x + \theta d\), where \(\theta \geq 0\).
Since costs decrease along the direction \(d\), the goal is to move as far as possible;
the farthest point will be \(x + \theta^\ast d\), where
\[ \theta^\ast = \max \left\{ \theta \geq 0 \mid x + \theta d \in P \right\}\]
The associated cost change is
\[ \Delta c = \theta^\ast c^T d = \theta^\ast \overline{c}_j \]

\bigskip
It's then necessary to determine the closed formula for \(\theta^\ast\).
Given that \(A d = 0\) \textit{(Equation~\ref{eq:feasible-direction-1})}, then \(A \left( x + \theta d \right) = Ax = b, \ \forall \, \theta\);
\(x + \theta d\) can become infeasible only if one of its component becomes negative, i.e. if \(x_i + \theta a_{ij} \leq 0\).
Two cases can be distinguished:

\begin{enumerate}
  \item \(d \geq 0\), then \(x + \theta d \geq 0\) for all \(\theta \geq 0\), the vector is never infeasible and \(\theta^\ast = \infty\)
  \item \(d_i \leq 0\) for some \(i\), then \(x_i + \theta d_i \geq 0 \Rightarrow \theta \leq -\sfrac{x_i}{d_i}\).
        Since this constraint must be satisfied for all \(i\), the largest possible value of \(\theta\) is
        \begin{equation}
          \theta^\ast = \min_{i=1, \, \ldots \,, m} \left\{ -\dfrac{x_i}{d_i} \,\middle\vert\, d_i \leq 0 \right\}
        \end{equation}
        Recalling that \(x_i\) is a non basic variable, then either \(x_i\) is the entering variable and \(d_i = 1\), or else \(d_i = 0\);
        in either case it's non negative and the formula can be re written as
        \begin{equation}
          \theta^\ast = \min_{i=1, \, \ldots \,, m} \left\{ - \dfrac{x_{B(i)}}{d_{B(i)}} \,\middle\vert\, d_{B(i)} < 0 \right\}
          \label{eq:theta-star}
        \end{equation}
        Note that \(\theta^\ast > 0\) since \(x_{B(i)} > 0 \ \forall \, i\).
\end{enumerate}

\subsubsection{Changes of basis}

\textbf{Assumption:} this Section refers to the case where the \LP is a minimization problem.

\bigskip
Once \(\theta^\ast\) has been chosen \textit{(Equation~\ref{eq:theta-star})}, assuming that's it's finite, the new feasible solution \(y= x + \theta^\ast d\) is obtained by moving along the basic direction \(d\) from \(x\).
Since \(x_j=0\) and \(d_j=1\), then \(y_j = \theta^\ast > 0\).

Let \(l\) be a minimizing index of \ref{eq:theta-star};
the following conditions must hold:

\begin{align*}
  - \dfrac{x_{B(l)}}{d_{B(l)}}    & = \min_{i=1, \, \ldots \,, m} \left\{ - \dfrac{x_{B(i)}}{d_{B(i)}} \,\middle\vert\, d_{B(i)} < 0 \right\} = \theta^\ast \\
  d_{B(l)}                        & < 0                                                                                                                     \\
  x_{B(l)} + \theta^\ast d_{B(l)} & = 0
\end{align*}

The basic variable \(x_{B(l)}\) is now \(0\), while the non basic variable \(x_j\) is \(y_j = \theta^\ast > 0\):
the basis has changed, and the new basis is \(\overline{B} = B \setminus \{ x_{B(l)} \} \cup \{ x_j \}\) \textit{(\(x_{B(l)}\) has left the basis, \(x_j\) has entered the basis)}.
The new matrix is now:
\[ \overline{B} = \left[ A_{B(1)} \mid  \cdots \mid A_{B(l-1)} \mid A_j \mid A_{B(l+1)} \mid \cdots \mid A_{B(m)} \right] \]

Equivalently, the set of basic indices \(B = \{ B(1), \, \ldots \,, B(m) \}\) can be updated as follows:
\[\overline{B}(i) = \begin{cases}
    B(i) \quad & \text{ if } i \neq l \\
    j \quad    & \text{ if } i = l
  \end{cases}\]

\bigskip
The previous steps can then be summarized in the following Theorem.

\begin{theorem}\hfill
  \begin{itemize}
    \item The columns \(A_{B(i)}\), \(i \neq l\) and \(A_j\) are linearly independent; \(\overline{B}\) is a basis matrix
    \item The vector \(y = x + \theta^\ast d\) is a basic feasible solution associated to the basis \(\overline{B}\)
  \end{itemize}
\end{theorem}

\paragraph{An iteration of the Simplex method}

A single iteration of the Simplex method consists of the following steps:

\begin{enumerate}[label=\arabic*., ref=(\arabic*)]
  \item \label{enum:simplex-iteration-1}start with a basis consisting of the basics columns \(A_{B(1)}, \, \ldots \,, A_{B(m)}\) and an associated basic feasible solution \(x\)
  \item \label{enum:simplex-iteration-2}compute the reduced costs \(\overline{c}_j\) of the non basic variables \(x_j\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item if the reduced costs are all non negative, then the current solution is optimal and the algorithm terminates
          \item otherwise, choose a non basic variable \(x_j\) with reduced cost \(\overline{c}_j \leq 0\)
        \end{itemize}
  \item \label{enum:simplex-iteration-3}compute \(u = B^{-1} A_j\) and the basic direction \(d\) associated to \(x_j\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item if no component of \(u\) is negative, then \(\theta^\ast = \infty\), the optimal cost is \(-\infty\) and the algorithm terminates
        \end{itemize}
  \item \label{enum:simplex-iteration-4}some components of \(u\) are negative, so \(\theta^\ast\) can be computed
        \[ \theta^\ast = \min_{i=1, \, \ldots \,, m} \left\{ - \dfrac{x_{B(i)}}{u_i} \,\middle\vert\, u_i < 0 \right\} \]
  \item \label{enum:simplex-iteration-5}let \(l\) be a minimizing index of Step~\ref{enum:simplex-iteration-4}; form a new basis replacing \(A_{B(l)}\) with \(A_j\).
        If \(y\) is the new basic feasible solution, then \(y_j = \theta^\ast\) and \(y_{B(i)} = x_{B(i)} + \theta^\ast u_i, i \neq l\)
\end{enumerate}

\bigskip
The method is initialized with an arbitrary basic feasible solution \(x\) and the corresponding basis \(B\);
the following Theorem \textit{(\ref{thm:simplex-iteration})} states that the algorithm terminates after a finite number of iterations.

\begin{theorem}
  Assume that the feasible set is non empty and that every basic feasible solution is non degenerate.
  Then, the Simplex method terminates after a finite number of iterations;
  at termination, there are two possibilities:
  \begin{enumerate}
    \item an optimal basis \(B\) and the optimal solution \(x\) is associated to it are found
    \item a vector \(d\) satisfying \(Ad = 0, d \geq 0, c^T d < 0\) is found, and the optimal cost is \(-\infty\)
  \end{enumerate}
  \label{thm:simplex-iteration}
\end{theorem}

\subsubsection{Pivoting operation}

The pivot operation is a single elementary row operation that transforms the inverse of the previous basis into the inverse of the new basis;
it's the same operation used in the Gaussian elimination method to solve systems of linear equations.

\bigskip
Given \(Ax = b\):

\begin{enumerate}
  \item \textbf{Select} a coefficient \(\overline{a}_{rs} \neq 0\), the \textbf{pivot}
  \item \textbf{Divide} the \(r\)-th row by \(\overline{a}_{rs}\)
  \item For each row \(i \neq r\) with \(\overline{a}_{rs} \neq 0\), \textbf{subtract the resulting \(r\)-th row multiplied by \(\overline{a}_{is}\)}
\end{enumerate}

This move does not affect the feasible solutions.
Only a finite number of pivots exists for each basis.

\bigskip
An example of pivot operation is shown in Figure~\ref{fig:pivot-operation-example}.

\begin{figure}[htbp]
  \centering
  \bigskip
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \bigskip
    \tikzfig{figure-15.tikz}
    \caption{Initial system}
    \label{fig:pivot-operation-example-1}
    \bigskip
  \end{subfigure}
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \bigskip
    \tikzfig{figure-16.tikz}
    \caption{Pivot operation}
    \bigskip
    \label{fig:pivot-operation-example-2}
  \end{subfigure}

  \caption{Pivot operation example}
  \label{fig:pivot-operation-example}
  \bigskip
\end{figure}

\subsubsection{Moving to an adjacent vertex}

\subparagraph*{Goals}
\begin{itemize}[label=\(\rightarrow\)]
  \item improve the objective function value
  \item preserve the feasibility
\end{itemize}

\bigskip
The general description of the Simplex method allows some freedom:
Step~\ref{enum:simplex-iteration-2} allows to choose any \(j\) whose reduced cost \(\overline{c}_j\) is negative and Step~\ref{enum:simplex-iteration-5} allows to choose any \(l\) minimizing \(\theta^\ast\).
Rules for making these choices are called \textbf{pivot rules}.

\begin{enumerate}
  \item Which non basic variable \textbf{enters} the basis?
        \begin{itemize}
          \item any one with reduced cost \(\overline{c}_j < 0\)
          \item the one yielding the maximum \(\Delta z = \theta^\ast \overline{c}_j\) with respect to the current basic feasible solution
          \item \textbf{Bland's rule}: \(s = \min \{ j \mid \overline{c}_j < 0 \}\)
                \begin{itemize}[label=\(\rightarrow\)]
                  \item \(\overline{c}_j > 0\) for maximization problems
                \end{itemize}
        \end{itemize}
  \item Which basic variable \textbf{leaves} the basis?
        \begin{itemize}
          \item \textbf{min ratio text}: index \(i\) with smallest positive ratio \(\dfrac{x_{B_i}}{\overline{a}_{is}} = \theta^\ast\) among those with \(\overline{a}_{is} > 0\)
          \item \textbf{Bland's rule}: \(r = \min \left\{ i \,\middle\vert\, \dfrac{\overline{b}_i}{\overline{a}{is}} = \theta^\ast \, \overline{a}_{is} > 0 \right\}\)
          \item also randomly
        \end{itemize}
\end{enumerate}

\bigskip
\begin{property}[Unboundedness]
  If the objective function is unbounded, the algorithm will never stop: if \(\exists \, \overline{c}_j < 0\) with \(\overline{a}_{ij} \leq 0 \, \forall \, i\) no element of the \(j\)-th column can play the role of the pivot.
  This condition is verified if problem is \textbf{unbounded}.
\end{property}

\subsubsection{Tableau representation}

\textbf{Tableau} is a matrix representation of the \LP problem.
Let
\[ \begin{matrix} z = c^T x \\ Ax = b \end{matrix}\] with implicit non negativity constraints be the \LP problem.

The initial tableau is the matrix represented in Figure~\ref{subfig:tableau-initial}.
The first column contains the right hand side of the objective function and the right hand side vector.

Consider a basis \(B\) and a partition \(A = \left[ B \ N \right]\), with \(0 = c^T x - z\).
The corresponding tableau is the matrix represented in Figure~\ref{subfig:tableau-basis}.

By pivoting operations \textit{(or pre multiplication by \(B^{-1}\))}, the tableau is in the canonical form with respect to \(B\).
The said tableau is the matrix represented in Figure~\ref{subfig:tableau-canonical}.

\begin{figure}[htbp]
  \centering
  \bigskip
  \begin{subfigure}[t]{0.99\textwidth}
    \centering
    \bigskip
    \tikzfig{figure-17.tikz}
    \caption{Initial tableau}
    \label{subfig:tableau-initial}
    \bigskip
  \end{subfigure}
  \bigskip
  \begin{subfigure}[b]{0.515\textwidth}
    \centering
    \bigskip
    \tikzfig{figure-18.tikz}
    \caption{Tableau with respect to a basis}
    \label{subfig:tableau-basis}
    \bigskip
  \end{subfigure}
  \begin{subfigure}[b]{0.475\textwidth}
    \centering
    \bigskip
    \tikzfig{figure-19.tikz}
    \caption{Canonical tableau}
    \label{subfig:tableau-canonical}
    \bigskip
  \end{subfigure}
  \caption{Tableau representation}
  \label{fig:tableau-representations}
  \bigskip
\end{figure}

\subsubsection{The simplex algorithm}

In Code~\ref{code:simplex} the simplex algorithm is implemented.

\begin{minipage}{0.99\textwidth}
  \begin{lstlisting}[caption={The simplex algorithm}, label={code:simplex}]
B[1], ..., B[m] := initial basis
construct the initial tableau A in canonical form with respect to B
unbounded := false
optimal := false
while optimal = false and unbounded = false do
  if a[0, j] $\forall$ j = 1, ..., m then
    optimal := true // for LP with min
  else
    select a non basic variable x_s with a[0, s] < 0 // negative reduced cost
    if a[i, s] $\leq$ 0 $\forall$ i = 1, ..., m then
      unbounded := true
    else
      r := argmin {a[i, 0] / a[i, s] $\forall$ i = 1, ..., m with a[i, s] > 0}
      pivot(r, s) // update the tableau
      B[r] := s
    end
  end
end
\end{lstlisting}
\end{minipage}

\subsubsection{Degenerate basic feasible solutions and convergence}

As already announced in Section~\ref{sec:basic-solutions}, a basic feasible solution \(x\) is degenerate if it contains at least one basic variable \(x_j = 0\).
A solution \(x\) with more than \(n -m\) zeroes corresponds to several distinct bases.

More than \(n\) constraints \textit{(the \(m\) of \(Ax = b\) and more than \(n-m\) among the \(n\) of \(x \geq 0\))} on the same vertex are satisfied with equality.
In the presence of degenerate basic feasible solutions \textit{(\BFS)}, a basis change may not decrease the objective function value:
if the current \BFS is degenerate, the only admissible value of \(\theta^\ast\) is \(0\) and the new \BFS is the same as the old one.

Note that a degenerate \BFS can arise from a non degenerate one:
even if \(\theta^\ast > 0\), several basic variables may go to \(0\) when \(x_s\) is increased to \(\theta^\ast\).
It's possible to cycle through a sequence of degenerate basis associated to the same vertex.

\begin{definition}[Degenerate Basic Solution]
  Consider the standard form polyhedron
  \[P = \left\{ x \in \R^n \mid   Ax = b, x \geq 0 \right\}\]
  and let \(x\) be a basic solution.
  Let \(m\) be the number of rows of \(A\).

  The vector \(x\) is a \textbf{degenerate basic solution} if more than \(n-m\) of the components of \(x\) are zero.
\end{definition}
\bigskip
A geometrical example of degenerate solution is the one in Figure~\ref{fig:degenerate-solutions}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{figure-20.tikz}
  \caption{\(A\) is a degenerate \BFS. \(B\) is a non degenerate \BFS.}
  \label{fig:degenerate-solutions}
  \centering
\end{figure}

\paragraph{Anti cycling rule}

Several anti cycling rules have bern proposed for the choice of the variables that enter end exit the bases \textit{(indices \(r\) and \(s\) in Algorithm~\ref{code:simplex})}.

The simplest one is the \textbf{Bland's rule}: among all candidate variables for \(x_s\) and \(x_r\), the one with the smallest index is chosen.

\begin{property}
  The simplex algorithm with Bland's rule terminates after less than \(\begin{pmatrix}
    n \\ m
  \end{pmatrix}\) iterations.
\end{property}

\subsubsection{Two phase simplex algorithm}

The two phase simplex algorithm is a modification of the simplex algorithm that allows to solve the \LP problem with equality
constraints.

\begin{enumerate}[label=Phase \arabic*:, ref=phase \arabic*, leftmargin=*, labelindent=1em, widest*=8]
  \item Determine an initial basic feasible solution
        \begin{itemize}
          \item given the problem with equality constraints
                \(\quad \begin{matrix}
                  z = c^T x \\
                  Ax = b    \\
                  x \geq 0
                \end{matrix}, b \geq 0\)
          \item an auxiliary \LP with artificial variables is constructed
                \(\quad \begin{matrix}
                  \min        & v = \displaystyle \sum_{i=1}^m y_1 \\
                  \text{s.t.} & Ax + ly = b                        \\
                              & x \geq 0, y \geq 0
                \end{matrix}\)
        \end{itemize}
  \item Solve the auxiliary problem
        \begin{itemize}
          \item if \(v^\ast > 0\), the problem is infeasible
          \item if \(v^\ast = 0\), \(y^\ast = 0\) and \(x^\ast\) is a basic feasible solution of the original problem
                \begin{itemize}
                  \item if \(y_i\) is non basic \(\forall \, i\), with \(1 \leq i \leq m\), the corresponding columns are deleted and a tableau in canonical form is obtained; the row of \(z\) must be determined via substitution
                  \item if there is a basic \(y_i\) \textit{(the basic feasible solution is degenerate)}, then a pivot operations is performed on the row of \(y_i\) to exchange the basic variable with a non basic one
                \end{itemize}
        \end{itemize}
\end{enumerate}

\subsection{Linear Programming duality}

To any \textit{maximization} \LP problem, a corresponding \textit{(with the same parameters)} \textit{minimization} problem can be associated;
in the same way, a \textit{minimization} problem can be associated to a \textit{maximization} problem.

The two problems are called \textbf{dual} of each other.
Despite having different spaces and objective functions, but in general the optimal objective function values coincide.

\bigskip
\begin{minipage}{0.3\textwidth}
  \begin{gather}
    \begin{aligned}
      \max        & \quad c^T x     \\
      \text{s.t.} & \quad Ax \leq b \\
                  & \quad x \geq 0
    \end{aligned}
    \label{eq:LP-duality-primal}
  \end{gather}
\end{minipage}
\hspace{0.3\textwidth}
\begin{minipage}{0.3\textwidth}
  \begin{gather}
    \begin{aligned}
      \min        & \quad b^T y        \\
      \text{s.t.} & \quad A^T y \geq c \\
                  & \quad y \geq 0
    \end{aligned}
    \label{eq:LP-duality-dual}
  \end{gather}
\end{minipage}

\bigskip
\begin{definition}
  The problem in Equation~\ref{eq:LP-duality-primal} is called the \textbf{primal problem} and the problem in Equation~\ref{eq:LP-duality-dual} is called the \textbf{dual problem}.
\end{definition}

\begin{property}
  The dual of the dual problem coincides with the primal problem.
\end{property}

\subsubsection{General transformation rules}

Table~\ref{tab:LP-duality-transformation-rules} summarizes the general transformation rules for the primal and dual problems.

\begin{table}[htbp]
  \centering
  \bigskip
  \begin{tblr}{colspec={c|c||c|c}, cells={valign=m}, rows={abovesep+=2pt, belowsep+=2pt}}
    \textbf{PRIMAL}      & \textit{minimize} & \textit{maximize} & \textbf{DUAL} \\
    \hline
    \textbf{constraints} & {\(\geq b_i\)                                         \\ \(\leq b_i\) \\ \(= b_i\)} & {\(\geq 0\) \\ \(\leq 0\) \\ free}              & \textbf{variables} \\
    \hline
    \textbf{variables}   & {\(\geq 0\)                                           \\ \(\leq 0\) \\ free} & {\(\leq c_j\) \\ \(\geq c_j\) \\ \(= c_j\)} & \textbf{constraints}
  \end{tblr}
  \bigskip
  \caption{Transformation rules for the dual problem}
  \label{tab:LP-duality-transformation-rules}
\end{table}

\subsubsection{The duality theorem}

\paragraph{Weak duality}

\begin{theorem}[Weak duality]
  If \(x\) is a feasible solution to the primal problem \textit{(Equation~\ref{eq:LP-duality-primal})} and \(y\) is a feasible solution to the dual problem \textit{(Equation~\ref{eq:LP-duality-dual})}, then
  \[ p^T b \leq c^T x \]
\end{theorem}

\begin{proof}
  For every pair \(x \in X\) and \(y \in Y\), where
  \[ X = \left\{  x \mid Ax \geq b, x \geq 0 \right\} \neq \emptyset \quad Y = \left\{ y \mid A^T y \leq c, y \geq 0 \right\} \neq \emptyset \]
  the following relations hold:
  \[ Ax \geq b, x \geq 0, A^T y \leq c, y \geq 0 \Rightarrow b^T y \leq x^T A^T y \leq x^T c = c^T x \]
\end{proof}

\subparagraph*{Consequences of the duality theorem}

If \(x \in X\) is a feasible solution of the primal problem \textit{(\ref{eq:LP-duality-primal})}, then \(y \in Y\) is a feasible solution of the dual problem \textit{(\ref{eq:LP-duality-dual})}, and the values of the respective objective functions coincide, \(c^T x = b^T y\), then \(x\) is optimal for the primal problem and \(y\) is optimal for the dual problem.
Optimal solutions are denoted by \(x^\ast\) and \(y^\ast\).

The following corollaries are immediate consequences of the duality theorem.

\begin{corollary}
  If the optimal cost in the primal problem is \(-\infty\), then the dual problem must be infeasible
\end{corollary}

\begin{corollary}
  If the optimal cost in the primal problem is \(+\infty\), then the dual problem must be infeasible
\end{corollary}

\paragraph{Strong duality}

\begin{theorem}[Strong duality]
  If \(X = \left\{ x \mid Ax \geq b, x \geq 0 \right\} \neq \emptyset\) and \(\min\left\{ c^T \mid x \in X \right\}\) is finite, there exists \(x^\ast \in X, \, y^\ast \in Y\) such that \(c^T x^\ast = b^T y^\ast\).
\end{theorem}

\begin{proof}
  Derive an optimal solution of the dual problem \textit{(\ref{eq:LP-duality-dual})} from one of the primal problem \textit{(\ref{eq:LP-duality-primal})} and
  \[ x^T = \begin{bmatrix}
      x_B^\ast \\ x_N^\ast
    \end{bmatrix} \quad x^\ast_B = B^{-1} b, \ x_N^\ast = 0 \]
  as optimal feasible solution of the primal problem, provided by the Simplex algorithm with the Bland's rule.

  Consider \(\overline{y}^T = c_B^T B^{-1}\).
  \begin{itemize}
    \item Verify that \(\overline{y}\) is a feasible solution of the dual problem:
          \begin{enumerate}
            \item for the non basic variables \[ \overline{c}_N^T = c_N^T - \left( c_B^T B^{-1} \right) N = c_N^T - \overline{y}^T N = \displaystyle \overbrace{\geq 0^T}^{x^\ast \ \text{optimal}} \Rightarrow \overline{y}^T N \leq c_N^T \]
            \item for the basic variables \[ \overline{c}_B^T = c_B^T - \left( c_B^T B^-1 \right) B = c_B^T - \overline{y}^T B = 0^T \Rightarrow y^T B \leq c_B^T \]
          \end{enumerate}
    \item According to the duality theorem, \[ \overline{y}^T b \leq c^T x^\ast \Rightarrow \overline{y}^T b = c^T x^\ast \]
  \end{itemize}

  Hence \(\overline{y}\) is an optimal solution of the dual problem: \(\overline{y} = y^\ast\).
\end{proof}

\subparagraph*{Consequences of the strong duality theorem}

Since in a linear programming problem, only one of the three following cases can occur:

\begin{itemize}
  \item There exists an optimal solution
  \item The problem is unbounded
        \begin{itemize}
          \item the optimal cost is \(+\infty\) for maximization problems
          \item the optimal cost is \(-\infty\) for minimization problems
        \end{itemize}
  \item The problem is infeasible
\end{itemize}

This leads to nine possible combinations of solutions of the primal and the dual problems;
all of them are shown in Table~\ref{tab:LP-duality-combinations}.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tblr}{colspec={rlc|c|c}, vline{3}={0.5pt}}
                                                       &                         & \SetCell[c=3]{c,b} \textit{dual}                                            \\
                                                       &                         & \textit{finite optimum}          & \textit{unbounded} & \textit{infeasible} \\
    \hline
    \SetCell[r=3]{c,b} \rotatebox{90}{\textit{primal}} & \textit{finite optimum} & \colorcmark                      & \colorxmark        & \colorxmark         \\
                                                       & \textit{unbounded}      & \colorxmark                      & \colorxmark        & \colorcmark         \\
                                                       & \textit{infeasible}     & \colorxmark                      & \colorcmark        & \colorcmark         \\
  \end{tblr}
  \bigskip
  \caption{Possible combinations of solutions of the primal and the dual}
  \label{tab:LP-duality-combinations}
\end{table}

\subsubsection{Complementary slackness conditions}

An important relation between primal and dual optimal solutions is provided by the complementary slackness conditions, shown in Theorem~\ref{thm:LP-duality-complementary-slackness}.

\begin{theorem}[Complementary slackness]
  Let \(x \in X^\ast\) and \(y \in Y^\ast\) be feasible solutions to the primal and dual problem, respectively.
  Then \(x\) and \(p\) are optimal for their respective problems if and only if
  \begin{gather*}
    \begin{aligned}
      y_i^\ast \left( a^T_i x - b_i \right) = 0 \quad & \forall \, i \in \left\{ i = 1, \, \ldots \,, m \right\} \\
      \left( c_j - p^T A_j \right) x_j^\ast = 0 \quad & \forall \, j \in \left\{ j = 1, \, \ldots \,, n \right\}
    \end{aligned}
  \end{gather*}
  where \(a_i\) denotes the \(i\)-th row of \(A\) and \(A_j\) the \(j\)-th column of \(A\).
  \label{thm:LP-duality-complementary-slackness}
\end{theorem}

At optimality, the product of each variable with the corresponding slack variable of the constraint of the relative dual problem must be zero.

\subsection{Sensitivity analysis}
\label{sec:sensitivity-analysis}

The sensitivity analysis of a linear programming problem is the study of the effect of small changes in the data of the problem on the optimal solution:
it evaluates the \inlinequote{sensitivity} of an optimal solution with respect to variations in the model parameters.

\begin{definition}[Shadow Price]
  The shadow price of the \(i\)-th resource is the maximum price the company is willing to pay to buy an additional unit of the \(i\)-th resource.
\end{definition}

\subsubsection{Algebraic Form}

Evaluate the sensitivity of an optimal solution when the model parameters \(\left\langle c_j, a_{ij}, b_i \right\rangle\) vary.
Given:

\begin{gather*}
  \begin{aligned}
    \min \,        & c^T x    \\
    \text{s.t.} \, & Ax = b   \\
                   & x \geq 0
  \end{aligned}
\end{gather*}

and optimal basic solution \(x^\ast\):

\begin{gather*}
  \begin{aligned}
    x_B^\ast & = B^{-1} b \geq 0 \\
    x_N^\ast & = 0
  \end{aligned}
\end{gather*}

The objective is to evaluate the limits that allow \(B\) to \textbf{remain optimal}, under two conditions:

\begin{enumerate}[itemsep=0.5ex]
  \item \textbf{feasibility}: \(B^{-1} b \geq 0\)
  \item \textbf{optimality}: \(c_N^T = c_N^T - c_B^T B^{-1} N = 0^T\)
\end{enumerate}

\paragraph[Changes in the requirement vector b]{Changes in the requirement vector \(b\)}

Suppose that some component \(b_i\) of the requirements vector \(b\) is changed by a small amount \(\delta\).
The new requirement vector is now:
\[ b^T \coloneqq b + \delta e_k \]
where \(e_k\) is the unitary vector equal to \(1\) on the \(k\)-th (\(1 \leq k \leq n\)) position and \(0\) elsewhere.

The objective is to determine the range of values of \(\delta\) under which the current basis remains optimal;
since the optimality conditions are not affected by the change, only the feasibility condition must be verified.

The basis \(B\) with the basic solution
\[ x^\ast = \begin{bmatrix} B^{-1} b^T \\ 0 \end{bmatrix} = \begin{bmatrix} B^{-1} \left( b + \delta e_k \right) \\ 0 \end{bmatrix} \]
remains optimal as long as the following relation holds:
\[ B^{-1} \left( b + \delta e_k \right) \geq 0 \Rightarrow B^{-1} b \geq - \delta B^{-1} e_k \]
The latter are \(m\) inequalities that define an interval of values for \(\delta \).
If \(\delta\) is outside the allowed range, then the current solution satisfies the optimality conditions, but is primal infeasible.

The basis \(B\) remains optimal, but the optimal basic feasible solution \(x^\ast\) changes.
The objective function value changes from \(c_B^T B^{-1} b\) to \(c_B^T B^{-1} \left( b + \delta e_k \right)\), and as such the optimal value of the dual variable is:
\[ \Delta z^\ast = \overbrace{c_B^T B^{-1}}^{y^{\ast T}} \left( \delta e_k \right) = \delta  y_k^\ast = \delta  \dfrac{\partial z^\ast}{\partial b_k}\]
where \(\Delta z^\ast\) is the \textbf{shadow price} of the \(k\)-th resource.

\paragraph[Changes in the cost vector c]{Changes in the cost vector \(c\)}

Suppose that some cost coefficient \(c_j\) of the objective function \(c\) is changed by a small amount \(\delta\).
The new cost vector is now:
\[ c^T \coloneqq c + \delta  e_j \]
where \(e_j\) is the unitary vector equal to \(1\) on the \(j\)-th (\(1 \leq j \leq n\)) position and \(0\) elsewhere.
The primal feasibility condition is not affected, so the only condition to verify is the optimality condition:
\[ c^T_B B^{-1} A \leq c^T \]

Two cases then arise:

\begin{enumerate}
  \item \(c_j\) is the cost of a basic variable: the optimal solution remains unchanged
  \item \(c_j\) is the cost of a non-basic variable: the optimal solution changes
\end{enumerate}

However, the basic solution is does not change:
\[ x_B^\ast = B^{-1} b \ \land x_N^\ast = 0 \]

\subparagraph*{Cost associated to a non variable}
If \(c_j\) is the cost coefficient of a \textbf{non basic variable} \(x\), then \(c_B\) is not affected and the only inequality affected is the one for the reduced cost of \(x_j\):
\[ c^T_B B^{-1} A_j \leq c_j + \delta \]
this inequality defines a lower bound of \(\delta\):
\[ \delta \geq - c_j \]
If this condition holds, the current basis is optimal.

The reduced cost represents the max decrease of \(c_j\) for which \(B\) remains optimal.

\subparagraph*{Cost associated to a basic variable}
If \(c_j\) is the cost coefficient of the \(l\)-th \textbf{basic variable}, then \(c_B\) is affected by the change in \(c_j\):
\[ c^T_B \coloneqq c_B + \delta  e_l \]
All the optimality conditions are then affected by the change in \(c_j\):
\[ \left( c_B + \delta e_l \right)^T B^{-1} A \leq c_i \quad \forall i \neq j \]
The variable \(x_j\) is skipped as it is basic and its reduced cost stays at zero.
The condition is equal to:
\[ \delta_{q_{li}} \leq c_i \quad \forall i \neq j \]
where \(q_{li}\) is the position of the \(l\)-th basic variable in the \(i\)-th row of \(A\).

These inequalities determine the range of \(\delta\) for which the same basis remains optimal.

\clearpage

\section{Integer Linear Programming}

\begin{definition}[Integer Linear Programming Problem]
  \label{def:integer-linear-programming-problem}
  An \textbf{Integer Linear Programming (\ILP) problem} is an optimization problem having form:
  \begin{align*}
    \min        & \quad c^T x      \\
    \text{s.t.} & \quad Ax = b     \\
                & \quad x \in \Z^n
  \end{align*}

  Furthermore:

  \begin{itemize}
    \item if \(x_j \in \left\{ 0, 1 \right\} \ \forall \, j\), the the problem is called \textbf{Binary LP}.
    \item if \(\exists \, i \ \text{such that} \ x_i \notin \Z\), then the problem is called \textbf{Mixed Integer LP}.
  \end{itemize}
\end{definition}

Note that the integrality condition \(x_i \in \Z\) is non linear, since it can be expressed as \(\sin(\pi x_j) = 0\).

\begin{definition}[Linear Relaxation]
  Let \(\ILP\) be an \ILP problem:
  \begin{align*}
    z_{\ILP} \coloneqq \max & \quad c^T x      \\
    \text{s.t.}             & \quad Ax \leq b  \\
                            & \quad x \in \Z^n \\
                            & \quad x \leq 0
  \end{align*}

  Then the \LP problem:
  \begin{align*}
    z_{\LP} \coloneqq \max & \quad c^T x     \\
    \text{s.t.}            & \quad Ax \leq b \\
                           & \quad x \leq 0
  \end{align*}

  is the \textbf{linear (or continuous) relaxation} of \(\ILP\).
\end{definition}

\begin{property}[Bounds of \ILP solutions]
  \label{prop:bounds-of-ilp-solutions}

  For any \ILP with \(\max\), the optimal solution is bounded by the optimal solution of the \LP relaxation:
  \[ z_{\ILP} \leq z_{\LP} \]

  For any \ILP with \(\min\), the optimal solution is bounded by the optimal solution of the \LP relaxation:
  \[ z_{\ILP} \geq z_{\LP} \]

\end{property}

\bigskip
The feasible region of any \ILP is a lattice of points, either finite or infinite according to the type of problem;
an illustration of this is given in Figure \ref{subfig:integer-lattice}.
By deleting the integrality constraint, the \ILP problem becomes a \LP problem, and the optimal solution of the \ILP problem isn't always the optimal solution of the \LP problem; such case is shown in Figure~\ref{subfig:integer-lattice-relaxed}.

\begin{figure}[htbp]
  \centering
  \bigskip
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \tikzfig{figure-23.tikz}
    \caption{Lattice of integer points}
    \label{subfig:integer-lattice}
  \end{subfigure}
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \tikzfig{figure-24.tikz}
    \caption{Relaxed Lattice of integer points}
    \label{subfig:integer-lattice-relaxed}
  \end{subfigure}
  \caption{Lattice of integer points}
  \label{fig:integer-lattice}
  \bigskip
\end{figure}

\subsection{Solutions of the \ILP problem}

\subparagraph*{Solutions as Relaxation of the \LP problem}
A plausible way to find a solution of the \ILP problem would be to find a solution of the \LP problem and then round it to the nearest integer point.
If an optimal solution of the \LP problem is integer, then it is also an optimal solution of the \ILP problem.

However, often the rounded optimal solutions of the \LP are either infeasible or

\begin{itemize}
  \item Infeasible solutions for the \ILP
  \item Useless solutions for the \ILP, as they are very different from an optimal solution of the \ILP
\end{itemize}

An illustration of this is given in Figure~\ref{fig:solution-relaxation-ilp}.

\begin{figure}[htbp]
  \centering
  \bigskip
  \tikzfig{figure-25.tikz}
  \caption{Solution of the \LP relaxation of the \ILP problem}
  \label{fig:solution-relaxation-ilp}
  \bigskip
\end{figure}

\subsubsection{Solution of Assignment and Transportation Problems}

Two important \ILP problems, namely the Assignment and Transportation problems, have a solution that is the optimal solution of the \LP relaxation;
they are described in Example~\ref{ex:assignment-problem} and Example~\ref{ex:transportation-problem} respectively.

\begin{example}[Assignment Problem]
  \label{ex:assignment-problem}

  Given:
  \begin{itemize}
    \item \(m\) machines, \(i = 1, \dots, m\)
    \item \(n\) jobs, \(j = 1, \dots, n\), \(n < m\)
    \item \(c_{ij}\) cost of assigning job \(j\) to machine \(i\)
  \end{itemize}
  determine an \textbf{assignment} of jobs to the machines so at to minimize the total cost, while assigning at least one job per machine and at most one machine for each job.

  Variables \(x_{ij}\) have value:
  \[ x_{ij} = \begin{cases}
      1 \quad & \text{if job } j \text{ is assigned to machine } i \\
      0 \quad & \text{otherwise}
    \end{cases} \]

  The \textbf{assignment problem} is the following \ILP problem:

  \begin{align*}
    \min        & \quad \sum_{i=1}^m \sum_{j=1}^n c_{ij} x_{ij}                                                      \\
    \text{s.t.} & \quad \sum_{i=1}^{m} x_{ij} = 1 \quad \forall \, j \quad \text{at most one machine for each job}   \\
                & \quad \sum_{j=1}^{n} x_{ij} = 1 \quad \forall \, i  \quad \text{at least one job for each machine} \\
                & \quad x_{ij} \in \left\{ 0, 1 \right\} \quad \forall \, i, j
  \end{align*}
\end{example}

\begin{example}[Transportation Problem]
  \label{ex:transportation-problem}
  Given:
  \begin{itemize}
    \item \(m\) productions plant, \(i = 1, \dots, m\)
    \item \(n\) clients, \(j = 1, \dots, n\), \(n > m\) by assumption
    \item \(c_{ij}\) cost of shipping one unit from plant \(i\) to client \(j\)
    \item \(p_i\) production capacity of plant \(i\)
    \item \(d_j\) demand of client \(j\)
    \item \(q_{ij} \geq 0\) quantity shipped from plant \(i\) to client \(j\)
  \end{itemize}
  determine a \textbf{transportation plan} that minimizes the total costs while satisfying the production and demand constraints.

  Assumption: \(\displaystyle \sum_{i=1}^m p_i = \geq \sum_{j=1}^{n} d_j\)

  Variables: \(x_{ij}\) quantity shipped from plant \(i\) to client \(j\)

\end{example}

The Assignment problem example contains a forcing constraint, while the Transportation problem adds a limit on the active number of variable;
the formal definitions are shown in Definition~\ref{def:forcing-constraint} and Definition~\ref{def:constraints-on-binary-variables} respectively.

\begin{definition}[Forcing constraint]
  \label{def:forcing-constraint}
  A constraint in the form
  \[ \displaystyle x \leq y\]
  is called a \textbf{forcing constraint} if both \(x\) and \(y\) are binary variables.
\end{definition}

\begin{definition}[Constraint on binary variables]
  \label{def:constraints-on-binary-variables}
  A constraint in the form
  \[ \displaystyle \sum_{i=1}^n x_i \leq 1 \]
  where all \(x_i\) are binary variables, implies that at most one of the variables \(x_i\) can be one.

  Similarly, a constraint in the form
  \[ \displaystyle \sum_{i=1}^n x_i = 1 \]
  where all \(x_i\) are binary variables, implies that exactly one variable \(x_i\) must be one.
\end{definition}

\paragraph{Solution of the transportation problem}

The Theorem~\ref{thm:transportation-problem} shows that the optimal solution of the \LP relaxation of the transportation problem is also the optimal solution of the \ILP problem.

\begin{theorem}[Solution of the Transportation Problem]
  \label{thm:transportation-problem}
  If in a transportation problem \(p_i, \ d_{ij}, \ q_{ij}\) are all integers, all the basic feasible solutions \textit{(vertices)} of its linear relaxation are integer.
\end{theorem}

\begin{proof}
  Let \(A\) be a integer constraint matrix of size \(\left( mn + n + m \right) \times \left( mn \right)\), where \(a_{ij} \in \left\{ -1, 0, 1 \right\}\).

  The right hand side vector \(b\) is composed of integer elements;
  the optimal solution of the linear relaxation is:

  \[ x^\ast = \begin{bmatrix}
      B^{-1} b \\ 0

    \end{bmatrix}
    \qquad
    B^{-1} = \dfrac{1}{|B|}
    \begin{bmatrix}
      \alpha_{11} & \dots  & \alpha_{1n} \\
      \ldots      & \ldots & \ldots      \\
      \alpha_{m1} & \dots  & \alpha_{mn}
    \end{bmatrix}
  \]

  where \(\alpha_{ij} = (-1)^{i+j} \det\left( M_{ij} \right)\), \(M_{ij}\) is the square submatrix obtained from \(B\) by deleting the \(i\)-th row and the \(j\)-th column.

  Then:
  \begin{itemize}
    \item \(B\) integer \(\Rightarrow\) \(\alpha_{ij}\) integer
    \item \(\det\left( B \right) = \pm 1 \ \Rightarrow \ B^{-1} \ \text{is integer} \ \Rightarrow \ x^\ast \ \text{is integer}\)
    \item It can be shown that \(A\) is totally unimodular, \(\Rightarrow \ \det\left( Q \right) = \left\{ -1, 0, 1 \right\}\) for any square submatrix \(Q\) of \(A\)
  \end{itemize}
\end{proof}

\paragraph{Complexity of the \ILP problem}

Most of the \ILP problems are \(\mathcal{NP}\)-hard:
an algorithm able to solve them and to prove that the solution is correct in polynomial time does not exists.

Different methods to find the optimal solutions exist, divided in:

\begin{itemize}
  \item \textbf{Implicit enumeration} methods
        \begin{itemize}
          \item[\cmarkthin] they provide an exact solution \textit{(a global optimum)}
          \item \textit{branch and bound} and \textit{dynamic programming} methods are part of this category
        \end{itemize}
  \item \textbf{Cutting planes} methods
        \begin{itemize}
          \item[\cmarkthin] they provide an exact solution \textit{(a global optimum)}
        \end{itemize}
  \item \textbf{Heuristic} algorithms
        \begin{itemize}
          \item[\xmarkthin] they provide an approximate solution \textit{(a local optimum)}
          \item \textit{greedy} and \textit{local search} algorithms are part of this category
        \end{itemize}
\end{itemize}

\subsection{Branch and Bound method}

The Branch and Bound method uses a \inlinequote{divide and conquer} strategy to solve the a \ILP problem, exploring its set of feasible solutions.

Let \(F\) be the set of feasible solution to the problem:
\begin{align*}
  \min       \quad & c^T     \\
  \text{s.t} \quad & x \in F \\
\end{align*}

The method is split in two phases:

\begin{enumerate}[label=Phase \arabic*., ref=step \arabic*, leftmargin=*, widest*=8, labelindent=1em]
  \item \textbf{branch}, the problem is partitioned in simpler subproblems
  \item \textbf{bound}, the subproblems are solved and the optimal solution is found
\end{enumerate}

The two phases are applied recursively until a solution is found;
if a problem is unsolvable in the bound phase, it is branched again until is simple enough to be solved.
This technique can be applied to a wide range of problems, not limited to \ILP problems.

\subparagraph*{Branching}

The set of solutions \(F\) is partitioned in \(k\) subsets:
\[ F = F_1 \cup \, \ldots \, \cup F_k \quad F_i \cap F_j = \emptyset \ \forall \, i \neq j \]
Let \(z_i\) be:
\[ z_i = \min\left\{ c(x) \,\middle\vert\, x \in F_i \right\} \quad i = 1, \, \ldots \, , k \]
The solution \(z\) is finally found as:
\[ z = \min\left\{ c(x) \,\middle\vert\, c \in F \right\} = \min\left\{ z_i, \, \ldots \,, z_k \right\} \]

\subparagraph*{Bounding}

For each subproblem \(\min\left\{ c(x) \,\middle\vert\, x \in F_i \right\}\), whose solution is found in the partition of \(F\), the optimal solution \(z_i\) is found by either:

\begin{enumerate}
  \item Determining an optimal solution of \(\min\left\{ c(x) \,\middle\vert\, x \in F_i \right\}\), \textbf{or}
  \item Proving that \(F_i = \emptyset\), \textbf{or}
  \item Proving that \(z_i \geq z\), where \(z\) is the optimal solution of the original problem found so far
\end{enumerate}

If the subproblem is not solved, it's mandatory to generate a new subproblem by branching it again.

\subparagraph*{Generic Branch and Bound Method}

If the lower bound \(b(F_i)\) of the optimal solution of the subproblem \(\min\left\{ c(x) \,\middle\vert\, x \in F_i \right\}\) is greater than the optimal solution \(z\) found so far, the subproblem is discarded, as it cannot contain a better solution than the one found so far.
At any point, the algorithm keeps in memory a set of active subproblems and the cost \(U\) of the best feasible solution found so far;
initially, \(U\) is either \(\infty\) or the cost of the best feasible solution found by the heuristic method (if any).

A typical step of the algorithm is:

\begin{enumerate}
  \item Select an active subproblem \(F_i\) to be solved
  \item If the subproblem is infeasible, delete it; otherwise, compute \(b(F_i)\) for the corresponding subproblem
  \item If \(b(F_i) \geq U\), delete \(F_i\)
  \item If \(F_i\) is feasible and \(b(F_i) < U\), either:
        \begin{itemize}
          \item obtain an optimal solution to the subproblem
          \item break the corresponding subproblem into further subproblems, which are added to the set of active subproblems?
        \end{itemize}
\end{enumerate}

\bigskip
There are several parameters that can be arbitrarily chosen;
there is not a fixed rule for many of them, as the best choice depends on the problem to be solved.

\begin{enumerate}
  \item There are multiple ways of choosing an active subproblem
  \item There are multiple ways of breaking a subproblem into further subproblems
  \item There are multiple ways of computing the lower bound \(b(F_i)\)
\end{enumerate}

\subsubsection{Branching Tree}

The branching tree is a tree that represents the branching process of the \ILP problem.
Each node of the tree represents a subproblem of the \ILP problem, and each edge represents a branching step.

A branching tree may not contain all possible nodes or all possible leaves \(2^d\);
a node of the tree has no child \textit{(is fathomed)} if:

\begin{enumerate}
  \item The initial constraint and those on the arcs from the root to the node are infeasible
  \item The optimal solution of the linear relaxation is integer
  \item The value \(C^T \overline{x}_{\text{LP}}\) of the optimal solution \(\overline{x}_{\text{LP}}\) of the linear relaxation is worse than the best feasible solution \(z\) found so far
\end{enumerate}

This criterion is called the \textbf{branching criterion}: it allows to discard a large number of nodes \textit{(subproblems)} that would not lead to a valid solution, thus reducing the size of the tree.

\bigskip
An example of a branching tree is shown in Figure~\ref{fig:branching-tree}: an edge represents a branching step, a node represents a subproblem of the \ILP problem, and the numbers on the second line  the node represent the values of the two variables \(x_1\) and \(x_2\).

\begin{figure}[htbp]of
  \centering
  \bigskip
  \tikzfig{figure-26.tikz}
  \caption{Branching tree}
  \label{fig:branching-tree}
  \bigskip
\end{figure}

\subsubsection{Branch and Bound for the \ILP problem}

Given an \ILP problem:
\[ \min\left\{ c^T x \,\mid\vert\, Ax = b, \ x \in \N \right\} \]
the Branch and Bound method can be applied to find the optimal solution \(z\).

\subparagraph*{Branching}

Partition \(F\) into subregions, let \(\overline{x}\) denote an optimal solution of the linear relaxation of the \ILP:
\[ \min\left\{ c^T x \,\mid\vert\, Ax = b, \ x \geq 0, \ x \in \R \right\} \quad z_{\text{LP}} = c^T \overline{x} \]
If \(\overline{x}\) is integer, then it's also optimal for the \ILP problem;
otherwise, the \ILP is partitioned into two subproblems:

\begin{gather}
  \min\left\{ c^T x \,\middle\vert\, Ax = b, x \leq \left\lfloor \overline{x} \right\rfloor \right\} \label{eq:branch1}\tag{ILP 1} \\
  \min\left\{ c^T x \,\middle\vert\, Ax = b, x \geq \left\lceil \overline{x} \right\rceil \right\} \label{eq:branch2}\tag{ILP 2}
\end{gather}

where the symbols \( \left\lfloor \overline{x} \right\rfloor \) and \( \left\lceil \overline{x} \right\rceil \) denote the floor and ceiling of the vector \(\overline{x}\).

\bigskip
Choosing the variable \(x_h\) whose fractional value is closest to \(0.5\), hoping to obtain two subproblems that are balanced is not always the best approach.
The best solution is the \textbf{strong branching}:
try to branch on some of the candidate variables (fractional basic ones) evaluating the corresponding objective function values and actually branch on the variable that yields the best improvement in the objective function value.

\subparagraph*{Bounding}

Determine a lower or upper bound, if the \ILP is respectively a minimization or a maximization problem on the optimal value \(z_i\) of a subproblem of \ILP by solving its linear relaxation.

\bigskip
In order to choose the node \textit{(subproblem)} to be solved, the following criteria can be used:

\begin{itemize}
  \item \textbf{Deeper nodes first} \textit{(depth-first search strategy)}: chooses the node with the greatest depth in the tree of subproblems
        \begin{itemize}
          \item[\cmarkthin] simple to implement, easy to optimize
          \item[\xmarkthin] may lead to a large tree of subproblems if the wrong branching variable is chosen
        \end{itemize}
  \item More promising nodes first \textit{(best-bound first strategy)}: chooses the node with the best linear relaxation value \(b(F_i)\)
        \begin{itemize}
          \item[\cmarkthin] generates a small tree of subproblems
          \item[\xmarkthin] the subproblems are less constrained, leading to longer times to find a feasible solution and improve it
        \end{itemize}
\end{itemize}

\paragraph{Remarks on the Branch and Bound method}

\subparagraph*{General remarks}
\begin{itemize}
  \item Branch and Bound is also applicable to mixed \ILP problems: at the branch step, just consider that the fractional variables are integer and the integer variables are continuous
  \item Finding a good initial feasible solution with an heuristic algorithm may improve the method's performance by providing a better lower or upper bound for \textit{(respectively)} maximization or minimization problems.
  \item Branch and Bound can also be used as a heuristic method for solving the \ILP problem: the algorithm stops when the best feasible solution found so far is better than the best integer solution found so far.
\end{itemize}

\subparagraph*{Efficient solution of the linear relaxation}
\begin{itemize}
  \item There's no need to solve the linear relaxations of the \ILP subproblem from scratch at each branching step: the solution of the linear relaxation of the parent node can be used as a starting point for the solution of the linear relaxation of the child node
  \item An optimal solution of the linear relaxation with a single additional constraint can be found via a single iteration of the \textbf{Dual Simplex method} \textit{(the simplex method applied to the dual problem)} to the optimal Branch and Bound of the previous node \textit{(linear relaxation of the parent subproblem)}
\end{itemize}

\subparagraph*{Applicability of Branch and Bound approach}

The Branch and Bound method can be adapted to solve any discrete optimization problem and many non linear optimization problems;
two procedures need to be changed:

\begin{itemize}
  \item The partition of the feasible region into subregions \textit{(branch)}
  \item The determination of a bound on the optimal value of the subproblem \textit{(bound)}
\end{itemize}

\end{document}